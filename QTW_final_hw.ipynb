{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc708829",
   "metadata": {},
   "source": [
    "# Final Homework.  \n",
    "- Babatunde John Olanipekun.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd32555",
   "metadata": {},
   "source": [
    "## About the dataset.  \n",
    "- We were not provided much information on the source of the dataset.  \n",
    "- But we know that 'y' is the target response and it is a binary attribute.  \n",
    "    - So this will be a classification task.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3214db22",
   "metadata": {},
   "source": [
    "## Approach.  \n",
    "- We will conduct usual data preprocessing.  \n",
    "- If we have more than 100,000 records (len(def)), we will split the data to train-test.  \n",
    "- Fit the data to logistic regression, random forest and XGBoost.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389d6702",
   "metadata": {},
   "source": [
    "## Evaluation.  \n",
    "- We will attempt to improve the worst model by 10% by further tuning the estimators.  \n",
    "- The target is > 90% accuracy on these estimators.  \n",
    "\n",
    "### Cost of prediction.  \n",
    "- False negative: 0 if 1 = USD1500.     \n",
    "- False positive: 1 if 0 = USD500.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65a3522d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51addbb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\olani\\\\OneDrive\\\\Documents\\\\Data Science\\\\SMU-Data Science\\\\Quantifying the world'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Confirm the working directory\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5742292e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'.\\final_project(5).csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "418f42ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f5685d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>...</th>\n",
       "      <th>x41</th>\n",
       "      <th>x42</th>\n",
       "      <th>x43</th>\n",
       "      <th>x44</th>\n",
       "      <th>x45</th>\n",
       "      <th>x46</th>\n",
       "      <th>x47</th>\n",
       "      <th>x48</th>\n",
       "      <th>x49</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.166563</td>\n",
       "      <td>-3.961588</td>\n",
       "      <td>4.621113</td>\n",
       "      <td>2.481908</td>\n",
       "      <td>-1.800135</td>\n",
       "      <td>0.804684</td>\n",
       "      <td>6.718751</td>\n",
       "      <td>-14.789997</td>\n",
       "      <td>-1.040673</td>\n",
       "      <td>-4.204950</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.497117</td>\n",
       "      <td>5.414063</td>\n",
       "      <td>-2.325655</td>\n",
       "      <td>1.674827</td>\n",
       "      <td>-0.264332</td>\n",
       "      <td>60.781427</td>\n",
       "      <td>-7.689696</td>\n",
       "      <td>0.151589</td>\n",
       "      <td>-8.040166</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.149894</td>\n",
       "      <td>-0.585676</td>\n",
       "      <td>27.839856</td>\n",
       "      <td>4.152333</td>\n",
       "      <td>6.426802</td>\n",
       "      <td>-2.426943</td>\n",
       "      <td>40.477058</td>\n",
       "      <td>-6.725709</td>\n",
       "      <td>0.896421</td>\n",
       "      <td>0.330165</td>\n",
       "      <td>...</td>\n",
       "      <td>36.292790</td>\n",
       "      <td>4.490915</td>\n",
       "      <td>0.762561</td>\n",
       "      <td>6.526662</td>\n",
       "      <td>1.007927</td>\n",
       "      <td>15.805696</td>\n",
       "      <td>-4.896678</td>\n",
       "      <td>-0.320283</td>\n",
       "      <td>16.719974</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.321707</td>\n",
       "      <td>-1.429819</td>\n",
       "      <td>12.251561</td>\n",
       "      <td>6.586874</td>\n",
       "      <td>-5.304647</td>\n",
       "      <td>-11.311090</td>\n",
       "      <td>17.812850</td>\n",
       "      <td>11.060572</td>\n",
       "      <td>5.325880</td>\n",
       "      <td>-2.632984</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.368491</td>\n",
       "      <td>9.088864</td>\n",
       "      <td>-0.689886</td>\n",
       "      <td>-2.731118</td>\n",
       "      <td>0.754200</td>\n",
       "      <td>30.856417</td>\n",
       "      <td>-7.428573</td>\n",
       "      <td>-2.090804</td>\n",
       "      <td>-7.869421</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.245594</td>\n",
       "      <td>5.076677</td>\n",
       "      <td>-24.149632</td>\n",
       "      <td>3.637307</td>\n",
       "      <td>6.505811</td>\n",
       "      <td>2.290224</td>\n",
       "      <td>-35.111751</td>\n",
       "      <td>-18.913592</td>\n",
       "      <td>-0.337041</td>\n",
       "      <td>-5.568076</td>\n",
       "      <td>...</td>\n",
       "      <td>15.691546</td>\n",
       "      <td>-7.467775</td>\n",
       "      <td>2.940789</td>\n",
       "      <td>-6.424112</td>\n",
       "      <td>0.419776</td>\n",
       "      <td>-72.424569</td>\n",
       "      <td>5.361375</td>\n",
       "      <td>1.806070</td>\n",
       "      <td>-7.670847</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.273366</td>\n",
       "      <td>0.306326</td>\n",
       "      <td>-11.352593</td>\n",
       "      <td>1.676758</td>\n",
       "      <td>2.928441</td>\n",
       "      <td>-0.616824</td>\n",
       "      <td>-16.505817</td>\n",
       "      <td>27.532281</td>\n",
       "      <td>1.199715</td>\n",
       "      <td>-4.309105</td>\n",
       "      <td>...</td>\n",
       "      <td>-13.911297</td>\n",
       "      <td>-5.229937</td>\n",
       "      <td>1.783928</td>\n",
       "      <td>3.957801</td>\n",
       "      <td>-0.096988</td>\n",
       "      <td>-14.085435</td>\n",
       "      <td>-0.208351</td>\n",
       "      <td>-0.894942</td>\n",
       "      <td>15.724742</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         x0        x1         x2        x3        x4         x5         x6  \\\n",
       "0 -0.166563 -3.961588   4.621113  2.481908 -1.800135   0.804684   6.718751   \n",
       "1 -0.149894 -0.585676  27.839856  4.152333  6.426802  -2.426943  40.477058   \n",
       "2 -0.321707 -1.429819  12.251561  6.586874 -5.304647 -11.311090  17.812850   \n",
       "3 -0.245594  5.076677 -24.149632  3.637307  6.505811   2.290224 -35.111751   \n",
       "4 -0.273366  0.306326 -11.352593  1.676758  2.928441  -0.616824 -16.505817   \n",
       "\n",
       "          x7        x8        x9  ...        x41       x42       x43  \\\n",
       "0 -14.789997 -1.040673 -4.204950  ...  -1.497117  5.414063 -2.325655   \n",
       "1  -6.725709  0.896421  0.330165  ...  36.292790  4.490915  0.762561   \n",
       "2  11.060572  5.325880 -2.632984  ...  -0.368491  9.088864 -0.689886   \n",
       "3 -18.913592 -0.337041 -5.568076  ...  15.691546 -7.467775  2.940789   \n",
       "4  27.532281  1.199715 -4.309105  ... -13.911297 -5.229937  1.783928   \n",
       "\n",
       "        x44       x45        x46       x47       x48        x49  y  \n",
       "0  1.674827 -0.264332  60.781427 -7.689696  0.151589  -8.040166  0  \n",
       "1  6.526662  1.007927  15.805696 -4.896678 -0.320283  16.719974  0  \n",
       "2 -2.731118  0.754200  30.856417 -7.428573 -2.090804  -7.869421  0  \n",
       "3 -6.424112  0.419776 -72.424569  5.361375  1.806070  -7.670847  0  \n",
       "4  3.957801 -0.096988 -14.085435 -0.208351 -0.894942  15.724742  1  \n",
       "\n",
       "[5 rows x 51 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# it is helpful to snoop into the dataset. \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2a98dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 160000 entries, 0 to 159999\n",
      "Data columns (total 51 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   x0      159974 non-null  float64\n",
      " 1   x1      159975 non-null  float64\n",
      " 2   x2      159962 non-null  float64\n",
      " 3   x3      159963 non-null  float64\n",
      " 4   x4      159974 non-null  float64\n",
      " 5   x5      159963 non-null  float64\n",
      " 6   x6      159974 non-null  float64\n",
      " 7   x7      159973 non-null  float64\n",
      " 8   x8      159979 non-null  float64\n",
      " 9   x9      159970 non-null  float64\n",
      " 10  x10     159957 non-null  float64\n",
      " 11  x11     159970 non-null  float64\n",
      " 12  x12     159964 non-null  float64\n",
      " 13  x13     159969 non-null  float64\n",
      " 14  x14     159966 non-null  float64\n",
      " 15  x15     159965 non-null  float64\n",
      " 16  x16     159974 non-null  float64\n",
      " 17  x17     159973 non-null  float64\n",
      " 18  x18     159960 non-null  float64\n",
      " 19  x19     159965 non-null  float64\n",
      " 20  x20     159962 non-null  float64\n",
      " 21  x21     159971 non-null  float64\n",
      " 22  x22     159973 non-null  float64\n",
      " 23  x23     159953 non-null  float64\n",
      " 24  x24     159972 non-null  object \n",
      " 25  x25     159978 non-null  float64\n",
      " 26  x26     159964 non-null  float64\n",
      " 27  x27     159970 non-null  float64\n",
      " 28  x28     159965 non-null  float64\n",
      " 29  x29     159970 non-null  object \n",
      " 30  x30     159970 non-null  object \n",
      " 31  x31     159961 non-null  float64\n",
      " 32  x32     159969 non-null  object \n",
      " 33  x33     159959 non-null  float64\n",
      " 34  x34     159959 non-null  float64\n",
      " 35  x35     159970 non-null  float64\n",
      " 36  x36     159973 non-null  float64\n",
      " 37  x37     159977 non-null  object \n",
      " 38  x38     159969 non-null  float64\n",
      " 39  x39     159977 non-null  float64\n",
      " 40  x40     159964 non-null  float64\n",
      " 41  x41     159960 non-null  float64\n",
      " 42  x42     159974 non-null  float64\n",
      " 43  x43     159963 non-null  float64\n",
      " 44  x44     159960 non-null  float64\n",
      " 45  x45     159971 non-null  float64\n",
      " 46  x46     159969 non-null  float64\n",
      " 47  x47     159963 non-null  float64\n",
      " 48  x48     159968 non-null  float64\n",
      " 49  x49     159968 non-null  float64\n",
      " 50  y       160000 non-null  int64  \n",
      "dtypes: float64(45), int64(1), object(5)\n",
      "memory usage: 62.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03bb2841",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>...</th>\n",
       "      <th>x41</th>\n",
       "      <th>x42</th>\n",
       "      <th>x43</th>\n",
       "      <th>x44</th>\n",
       "      <th>x45</th>\n",
       "      <th>x46</th>\n",
       "      <th>x47</th>\n",
       "      <th>x48</th>\n",
       "      <th>x49</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>159974.000000</td>\n",
       "      <td>159975.000000</td>\n",
       "      <td>159962.000000</td>\n",
       "      <td>159963.000000</td>\n",
       "      <td>159974.000000</td>\n",
       "      <td>159963.000000</td>\n",
       "      <td>159974.000000</td>\n",
       "      <td>159973.000000</td>\n",
       "      <td>159979.000000</td>\n",
       "      <td>159970.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>159960.000000</td>\n",
       "      <td>159974.000000</td>\n",
       "      <td>159963.000000</td>\n",
       "      <td>159960.000000</td>\n",
       "      <td>159971.000000</td>\n",
       "      <td>159969.000000</td>\n",
       "      <td>159963.000000</td>\n",
       "      <td>159968.000000</td>\n",
       "      <td>159968.000000</td>\n",
       "      <td>160000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.001028</td>\n",
       "      <td>0.001358</td>\n",
       "      <td>-1.150145</td>\n",
       "      <td>-0.024637</td>\n",
       "      <td>-0.000549</td>\n",
       "      <td>0.013582</td>\n",
       "      <td>-1.670670</td>\n",
       "      <td>-7.692795</td>\n",
       "      <td>-0.030540</td>\n",
       "      <td>0.005462</td>\n",
       "      <td>...</td>\n",
       "      <td>6.701076</td>\n",
       "      <td>-1.833820</td>\n",
       "      <td>-0.002091</td>\n",
       "      <td>-0.006250</td>\n",
       "      <td>0.000885</td>\n",
       "      <td>-12.755395</td>\n",
       "      <td>0.028622</td>\n",
       "      <td>-0.000224</td>\n",
       "      <td>-0.674224</td>\n",
       "      <td>0.401231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.371137</td>\n",
       "      <td>6.340632</td>\n",
       "      <td>13.273480</td>\n",
       "      <td>8.065032</td>\n",
       "      <td>6.382293</td>\n",
       "      <td>7.670076</td>\n",
       "      <td>19.298665</td>\n",
       "      <td>30.542264</td>\n",
       "      <td>8.901185</td>\n",
       "      <td>6.355040</td>\n",
       "      <td>...</td>\n",
       "      <td>18.680196</td>\n",
       "      <td>5.110705</td>\n",
       "      <td>1.534952</td>\n",
       "      <td>4.164595</td>\n",
       "      <td>0.396621</td>\n",
       "      <td>36.608641</td>\n",
       "      <td>4.788157</td>\n",
       "      <td>1.935501</td>\n",
       "      <td>15.036738</td>\n",
       "      <td>0.490149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.592635</td>\n",
       "      <td>-26.278302</td>\n",
       "      <td>-59.394048</td>\n",
       "      <td>-35.476594</td>\n",
       "      <td>-28.467536</td>\n",
       "      <td>-33.822988</td>\n",
       "      <td>-86.354483</td>\n",
       "      <td>-181.506976</td>\n",
       "      <td>-37.691045</td>\n",
       "      <td>-27.980659</td>\n",
       "      <td>...</td>\n",
       "      <td>-82.167224</td>\n",
       "      <td>-27.933750</td>\n",
       "      <td>-6.876234</td>\n",
       "      <td>-17.983487</td>\n",
       "      <td>-1.753221</td>\n",
       "      <td>-201.826828</td>\n",
       "      <td>-21.086333</td>\n",
       "      <td>-8.490155</td>\n",
       "      <td>-65.791191</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.251641</td>\n",
       "      <td>-4.260973</td>\n",
       "      <td>-10.166536</td>\n",
       "      <td>-5.454438</td>\n",
       "      <td>-4.313118</td>\n",
       "      <td>-5.148130</td>\n",
       "      <td>-14.780146</td>\n",
       "      <td>-27.324771</td>\n",
       "      <td>-6.031058</td>\n",
       "      <td>-4.260619</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.804080</td>\n",
       "      <td>-5.162869</td>\n",
       "      <td>-1.039677</td>\n",
       "      <td>-2.812055</td>\n",
       "      <td>-0.266518</td>\n",
       "      <td>-36.428329</td>\n",
       "      <td>-3.216016</td>\n",
       "      <td>-1.320800</td>\n",
       "      <td>-10.931753</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.002047</td>\n",
       "      <td>0.004813</td>\n",
       "      <td>-1.340932</td>\n",
       "      <td>-0.031408</td>\n",
       "      <td>0.000857</td>\n",
       "      <td>0.014118</td>\n",
       "      <td>-1.948594</td>\n",
       "      <td>-6.956789</td>\n",
       "      <td>-0.016840</td>\n",
       "      <td>0.006045</td>\n",
       "      <td>...</td>\n",
       "      <td>6.840110</td>\n",
       "      <td>-1.923754</td>\n",
       "      <td>-0.004385</td>\n",
       "      <td>-0.010484</td>\n",
       "      <td>0.001645</td>\n",
       "      <td>-12.982497</td>\n",
       "      <td>0.035865</td>\n",
       "      <td>-0.011993</td>\n",
       "      <td>-0.574410</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.248532</td>\n",
       "      <td>4.284220</td>\n",
       "      <td>7.871676</td>\n",
       "      <td>5.445179</td>\n",
       "      <td>4.306660</td>\n",
       "      <td>5.190749</td>\n",
       "      <td>11.446931</td>\n",
       "      <td>12.217071</td>\n",
       "      <td>5.972349</td>\n",
       "      <td>4.305734</td>\n",
       "      <td>...</td>\n",
       "      <td>19.266367</td>\n",
       "      <td>1.453507</td>\n",
       "      <td>1.033275</td>\n",
       "      <td>2.783274</td>\n",
       "      <td>0.269049</td>\n",
       "      <td>11.445443</td>\n",
       "      <td>3.268028</td>\n",
       "      <td>1.317703</td>\n",
       "      <td>9.651072</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.600849</td>\n",
       "      <td>27.988178</td>\n",
       "      <td>63.545653</td>\n",
       "      <td>38.906025</td>\n",
       "      <td>26.247812</td>\n",
       "      <td>35.550110</td>\n",
       "      <td>92.390605</td>\n",
       "      <td>149.150634</td>\n",
       "      <td>39.049831</td>\n",
       "      <td>27.377842</td>\n",
       "      <td>...</td>\n",
       "      <td>100.050432</td>\n",
       "      <td>22.668041</td>\n",
       "      <td>6.680922</td>\n",
       "      <td>19.069759</td>\n",
       "      <td>1.669205</td>\n",
       "      <td>150.859415</td>\n",
       "      <td>20.836854</td>\n",
       "      <td>8.226552</td>\n",
       "      <td>66.877604</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  x0             x1             x2             x3  \\\n",
       "count  159974.000000  159975.000000  159962.000000  159963.000000   \n",
       "mean       -0.001028       0.001358      -1.150145      -0.024637   \n",
       "std         0.371137       6.340632      13.273480       8.065032   \n",
       "min        -1.592635     -26.278302     -59.394048     -35.476594   \n",
       "25%        -0.251641      -4.260973     -10.166536      -5.454438   \n",
       "50%        -0.002047       0.004813      -1.340932      -0.031408   \n",
       "75%         0.248532       4.284220       7.871676       5.445179   \n",
       "max         1.600849      27.988178      63.545653      38.906025   \n",
       "\n",
       "                  x4             x5             x6             x7  \\\n",
       "count  159974.000000  159963.000000  159974.000000  159973.000000   \n",
       "mean       -0.000549       0.013582      -1.670670      -7.692795   \n",
       "std         6.382293       7.670076      19.298665      30.542264   \n",
       "min       -28.467536     -33.822988     -86.354483    -181.506976   \n",
       "25%        -4.313118      -5.148130     -14.780146     -27.324771   \n",
       "50%         0.000857       0.014118      -1.948594      -6.956789   \n",
       "75%         4.306660       5.190749      11.446931      12.217071   \n",
       "max        26.247812      35.550110      92.390605     149.150634   \n",
       "\n",
       "                  x8             x9  ...            x41            x42  \\\n",
       "count  159979.000000  159970.000000  ...  159960.000000  159974.000000   \n",
       "mean       -0.030540       0.005462  ...       6.701076      -1.833820   \n",
       "std         8.901185       6.355040  ...      18.680196       5.110705   \n",
       "min       -37.691045     -27.980659  ...     -82.167224     -27.933750   \n",
       "25%        -6.031058      -4.260619  ...      -5.804080      -5.162869   \n",
       "50%        -0.016840       0.006045  ...       6.840110      -1.923754   \n",
       "75%         5.972349       4.305734  ...      19.266367       1.453507   \n",
       "max        39.049831      27.377842  ...     100.050432      22.668041   \n",
       "\n",
       "                 x43            x44            x45            x46  \\\n",
       "count  159963.000000  159960.000000  159971.000000  159969.000000   \n",
       "mean       -0.002091      -0.006250       0.000885     -12.755395   \n",
       "std         1.534952       4.164595       0.396621      36.608641   \n",
       "min        -6.876234     -17.983487      -1.753221    -201.826828   \n",
       "25%        -1.039677      -2.812055      -0.266518     -36.428329   \n",
       "50%        -0.004385      -0.010484       0.001645     -12.982497   \n",
       "75%         1.033275       2.783274       0.269049      11.445443   \n",
       "max         6.680922      19.069759       1.669205     150.859415   \n",
       "\n",
       "                 x47            x48            x49              y  \n",
       "count  159963.000000  159968.000000  159968.000000  160000.000000  \n",
       "mean        0.028622      -0.000224      -0.674224       0.401231  \n",
       "std         4.788157       1.935501      15.036738       0.490149  \n",
       "min       -21.086333      -8.490155     -65.791191       0.000000  \n",
       "25%        -3.216016      -1.320800     -10.931753       0.000000  \n",
       "50%         0.035865      -0.011993      -0.574410       0.000000  \n",
       "75%         3.268028       1.317703       9.651072       1.000000  \n",
       "max        20.836854       8.226552      66.877604       1.000000  \n",
       "\n",
       "[8 rows x 46 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4896c19e",
   "metadata": {},
   "source": [
    "The mean and median in most variable are quite close which suggests that these variables are nearly normally distributed.  \n",
    "So we may not require any normalization step.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "792ecc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "df1 = copy.deepcopy(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16f942c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x32</th>\n",
       "      <th>x33</th>\n",
       "      <th>x34</th>\n",
       "      <th>x35</th>\n",
       "      <th>x36</th>\n",
       "      <th>x37</th>\n",
       "      <th>x38</th>\n",
       "      <th>x39</th>\n",
       "      <th>x40</th>\n",
       "      <th>x41</th>\n",
       "      <th>x42</th>\n",
       "      <th>x43</th>\n",
       "      <th>x44</th>\n",
       "      <th>x45</th>\n",
       "      <th>x46</th>\n",
       "      <th>x47</th>\n",
       "      <th>x48</th>\n",
       "      <th>x49</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0%</td>\n",
       "      <td>-1.940031</td>\n",
       "      <td>-5.492063</td>\n",
       "      <td>0.627121</td>\n",
       "      <td>-0.873824</td>\n",
       "      <td>$1313.96</td>\n",
       "      <td>-1.353729</td>\n",
       "      <td>-5.186148</td>\n",
       "      <td>-10.612200</td>\n",
       "      <td>-1.497117</td>\n",
       "      <td>5.414063</td>\n",
       "      <td>-2.325655</td>\n",
       "      <td>1.674827</td>\n",
       "      <td>-0.264332</td>\n",
       "      <td>60.781427</td>\n",
       "      <td>-7.689696</td>\n",
       "      <td>0.151589</td>\n",
       "      <td>-8.040166</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.02%</td>\n",
       "      <td>2.211462</td>\n",
       "      <td>-4.460591</td>\n",
       "      <td>1.035461</td>\n",
       "      <td>0.228270</td>\n",
       "      <td>$1962.78</td>\n",
       "      <td>32.816804</td>\n",
       "      <td>-5.150012</td>\n",
       "      <td>2.147427</td>\n",
       "      <td>36.292790</td>\n",
       "      <td>4.490915</td>\n",
       "      <td>0.762561</td>\n",
       "      <td>6.526662</td>\n",
       "      <td>1.007927</td>\n",
       "      <td>15.805696</td>\n",
       "      <td>-4.896678</td>\n",
       "      <td>-0.320283</td>\n",
       "      <td>16.719974</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.01%</td>\n",
       "      <td>0.419607</td>\n",
       "      <td>-3.804056</td>\n",
       "      <td>-0.763357</td>\n",
       "      <td>-1.612561</td>\n",
       "      <td>$430.47</td>\n",
       "      <td>-0.333199</td>\n",
       "      <td>8.728585</td>\n",
       "      <td>-0.863137</td>\n",
       "      <td>-0.368491</td>\n",
       "      <td>9.088864</td>\n",
       "      <td>-0.689886</td>\n",
       "      <td>-2.731118</td>\n",
       "      <td>0.754200</td>\n",
       "      <td>30.856417</td>\n",
       "      <td>-7.428573</td>\n",
       "      <td>-2.090804</td>\n",
       "      <td>-7.869421</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.01%</td>\n",
       "      <td>-3.442715</td>\n",
       "      <td>4.420160</td>\n",
       "      <td>1.164532</td>\n",
       "      <td>3.033455</td>\n",
       "      <td>$-2366.29</td>\n",
       "      <td>14.188669</td>\n",
       "      <td>-6.385060</td>\n",
       "      <td>12.084421</td>\n",
       "      <td>15.691546</td>\n",
       "      <td>-7.467775</td>\n",
       "      <td>2.940789</td>\n",
       "      <td>-6.424112</td>\n",
       "      <td>0.419776</td>\n",
       "      <td>-72.424569</td>\n",
       "      <td>5.361375</td>\n",
       "      <td>1.806070</td>\n",
       "      <td>-7.670847</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.01%</td>\n",
       "      <td>-0.431640</td>\n",
       "      <td>12.165494</td>\n",
       "      <td>-0.167726</td>\n",
       "      <td>-0.341604</td>\n",
       "      <td>$-620.66</td>\n",
       "      <td>-12.578926</td>\n",
       "      <td>1.133798</td>\n",
       "      <td>30.004727</td>\n",
       "      <td>-13.911297</td>\n",
       "      <td>-5.229937</td>\n",
       "      <td>1.783928</td>\n",
       "      <td>3.957801</td>\n",
       "      <td>-0.096988</td>\n",
       "      <td>-14.085435</td>\n",
       "      <td>-0.208351</td>\n",
       "      <td>-0.894942</td>\n",
       "      <td>15.724742</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.01%</td>\n",
       "      <td>0.962813</td>\n",
       "      <td>2.934661</td>\n",
       "      <td>-2.725477</td>\n",
       "      <td>0.656132</td>\n",
       "      <td>$-196.45</td>\n",
       "      <td>-6.555903</td>\n",
       "      <td>6.014648</td>\n",
       "      <td>3.091261</td>\n",
       "      <td>-7.250310</td>\n",
       "      <td>6.402343</td>\n",
       "      <td>-2.064860</td>\n",
       "      <td>0.923879</td>\n",
       "      <td>0.331452</td>\n",
       "      <td>19.172365</td>\n",
       "      <td>5.752749</td>\n",
       "      <td>-2.609553</td>\n",
       "      <td>-20.320179</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.01%</td>\n",
       "      <td>1.487652</td>\n",
       "      <td>19.546647</td>\n",
       "      <td>-4.279037</td>\n",
       "      <td>-0.000735</td>\n",
       "      <td>$-241.04</td>\n",
       "      <td>-14.102935</td>\n",
       "      <td>-6.139318</td>\n",
       "      <td>-8.818327</td>\n",
       "      <td>-15.596731</td>\n",
       "      <td>-7.779833</td>\n",
       "      <td>-1.153986</td>\n",
       "      <td>-1.081095</td>\n",
       "      <td>-0.538983</td>\n",
       "      <td>1.573406</td>\n",
       "      <td>2.102627</td>\n",
       "      <td>-2.966103</td>\n",
       "      <td>4.604337</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.01%</td>\n",
       "      <td>0.467617</td>\n",
       "      <td>10.537118</td>\n",
       "      <td>0.061739</td>\n",
       "      <td>0.010240</td>\n",
       "      <td>$621.35</td>\n",
       "      <td>17.885994</td>\n",
       "      <td>11.162321</td>\n",
       "      <td>-2.479899</td>\n",
       "      <td>19.780494</td>\n",
       "      <td>-3.711105</td>\n",
       "      <td>-3.134008</td>\n",
       "      <td>-5.805255</td>\n",
       "      <td>0.966762</td>\n",
       "      <td>6.639034</td>\n",
       "      <td>6.258786</td>\n",
       "      <td>1.272556</td>\n",
       "      <td>13.186184</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.03%</td>\n",
       "      <td>-0.704558</td>\n",
       "      <td>-9.100507</td>\n",
       "      <td>6.544267</td>\n",
       "      <td>-1.951658</td>\n",
       "      <td>$-301.89</td>\n",
       "      <td>-23.704439</td>\n",
       "      <td>7.594268</td>\n",
       "      <td>7.576010</td>\n",
       "      <td>-26.215235</td>\n",
       "      <td>0.330604</td>\n",
       "      <td>-2.015631</td>\n",
       "      <td>2.219910</td>\n",
       "      <td>0.533815</td>\n",
       "      <td>-2.627660</td>\n",
       "      <td>0.182319</td>\n",
       "      <td>3.140230</td>\n",
       "      <td>14.495677</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.01%</td>\n",
       "      <td>2.792562</td>\n",
       "      <td>-5.347096</td>\n",
       "      <td>1.084512</td>\n",
       "      <td>1.479771</td>\n",
       "      <td>$-484.09</td>\n",
       "      <td>17.421890</td>\n",
       "      <td>-0.302757</td>\n",
       "      <td>-9.152182</td>\n",
       "      <td>19.267232</td>\n",
       "      <td>-0.675888</td>\n",
       "      <td>-2.560935</td>\n",
       "      <td>1.326146</td>\n",
       "      <td>0.444997</td>\n",
       "      <td>-20.942887</td>\n",
       "      <td>-3.141996</td>\n",
       "      <td>-1.608473</td>\n",
       "      <td>7.169219</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.01%</td>\n",
       "      <td>-3.280432</td>\n",
       "      <td>5.477774</td>\n",
       "      <td>4.298599</td>\n",
       "      <td>1.616987</td>\n",
       "      <td>$-106.65</td>\n",
       "      <td>4.546234</td>\n",
       "      <td>-2.775233</td>\n",
       "      <td>14.001419</td>\n",
       "      <td>5.027775</td>\n",
       "      <td>-3.903575</td>\n",
       "      <td>1.938380</td>\n",
       "      <td>-5.395413</td>\n",
       "      <td>0.459957</td>\n",
       "      <td>-77.491333</td>\n",
       "      <td>0.754309</td>\n",
       "      <td>-0.442017</td>\n",
       "      <td>-21.824215</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.02%</td>\n",
       "      <td>2.213868</td>\n",
       "      <td>1.571707</td>\n",
       "      <td>1.239489</td>\n",
       "      <td>0.534812</td>\n",
       "      <td>$-622.76</td>\n",
       "      <td>-12.063685</td>\n",
       "      <td>-2.632316</td>\n",
       "      <td>7.565319</td>\n",
       "      <td>-13.341481</td>\n",
       "      <td>-8.749303</td>\n",
       "      <td>-2.927836</td>\n",
       "      <td>-4.118254</td>\n",
       "      <td>0.790090</td>\n",
       "      <td>-95.290753</td>\n",
       "      <td>-2.076083</td>\n",
       "      <td>0.841078</td>\n",
       "      <td>-16.644318</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.01%</td>\n",
       "      <td>-2.241071</td>\n",
       "      <td>-15.237309</td>\n",
       "      <td>0.629570</td>\n",
       "      <td>0.056979</td>\n",
       "      <td>$545.55</td>\n",
       "      <td>26.729089</td>\n",
       "      <td>0.979735</td>\n",
       "      <td>-12.285052</td>\n",
       "      <td>29.560258</td>\n",
       "      <td>-5.199020</td>\n",
       "      <td>-2.492575</td>\n",
       "      <td>6.754616</td>\n",
       "      <td>-0.951078</td>\n",
       "      <td>-73.732913</td>\n",
       "      <td>-3.624559</td>\n",
       "      <td>1.414010</td>\n",
       "      <td>-25.409134</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.01%</td>\n",
       "      <td>-0.522456</td>\n",
       "      <td>-12.537868</td>\n",
       "      <td>-0.473894</td>\n",
       "      <td>-0.931481</td>\n",
       "      <td>$1019.08</td>\n",
       "      <td>20.820001</td>\n",
       "      <td>6.666158</td>\n",
       "      <td>-11.567399</td>\n",
       "      <td>23.025274</td>\n",
       "      <td>-8.812971</td>\n",
       "      <td>-0.233724</td>\n",
       "      <td>-4.021603</td>\n",
       "      <td>-0.573178</td>\n",
       "      <td>5.543237</td>\n",
       "      <td>-4.584788</td>\n",
       "      <td>-0.376251</td>\n",
       "      <td>3.194482</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.0%</td>\n",
       "      <td>-0.708466</td>\n",
       "      <td>-6.669252</td>\n",
       "      <td>-0.330493</td>\n",
       "      <td>1.258759</td>\n",
       "      <td>$960.95</td>\n",
       "      <td>29.194515</td>\n",
       "      <td>5.363980</td>\n",
       "      <td>-8.897842</td>\n",
       "      <td>32.286825</td>\n",
       "      <td>0.724720</td>\n",
       "      <td>0.702446</td>\n",
       "      <td>-1.832473</td>\n",
       "      <td>-0.092212</td>\n",
       "      <td>8.723096</td>\n",
       "      <td>-1.059007</td>\n",
       "      <td>2.602944</td>\n",
       "      <td>19.141027</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.01%</td>\n",
       "      <td>-1.238546</td>\n",
       "      <td>-5.213736</td>\n",
       "      <td>2.255307</td>\n",
       "      <td>2.445620</td>\n",
       "      <td>$299.52</td>\n",
       "      <td>23.654955</td>\n",
       "      <td>4.497487</td>\n",
       "      <td>-52.283301</td>\n",
       "      <td>26.160509</td>\n",
       "      <td>0.290090</td>\n",
       "      <td>-0.984782</td>\n",
       "      <td>1.954579</td>\n",
       "      <td>0.383549</td>\n",
       "      <td>-1.467533</td>\n",
       "      <td>13.830803</td>\n",
       "      <td>-2.831817</td>\n",
       "      <td>9.343167</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.01%</td>\n",
       "      <td>1.323644</td>\n",
       "      <td>2.647041</td>\n",
       "      <td>-1.229748</td>\n",
       "      <td>0.479819</td>\n",
       "      <td>$72.83</td>\n",
       "      <td>9.572061</td>\n",
       "      <td>-7.938113</td>\n",
       "      <td>-12.132429</td>\n",
       "      <td>10.585942</td>\n",
       "      <td>-6.545866</td>\n",
       "      <td>1.088224</td>\n",
       "      <td>-2.185282</td>\n",
       "      <td>-0.230979</td>\n",
       "      <td>21.335008</td>\n",
       "      <td>-1.517562</td>\n",
       "      <td>-0.445338</td>\n",
       "      <td>9.285682</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.0%</td>\n",
       "      <td>2.908810</td>\n",
       "      <td>-9.367590</td>\n",
       "      <td>-4.144378</td>\n",
       "      <td>-1.585798</td>\n",
       "      <td>$-12.09</td>\n",
       "      <td>-14.349460</td>\n",
       "      <td>-3.660668</td>\n",
       "      <td>19.549106</td>\n",
       "      <td>-15.869368</td>\n",
       "      <td>-1.301406</td>\n",
       "      <td>-2.784866</td>\n",
       "      <td>3.139330</td>\n",
       "      <td>-0.330494</td>\n",
       "      <td>-28.760882</td>\n",
       "      <td>-2.793687</td>\n",
       "      <td>0.801223</td>\n",
       "      <td>-2.128184</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0%</td>\n",
       "      <td>-2.902935</td>\n",
       "      <td>14.768572</td>\n",
       "      <td>-2.019176</td>\n",
       "      <td>-0.940080</td>\n",
       "      <td>$1074.27</td>\n",
       "      <td>7.066684</td>\n",
       "      <td>-0.915279</td>\n",
       "      <td>6.367554</td>\n",
       "      <td>7.815194</td>\n",
       "      <td>1.138352</td>\n",
       "      <td>0.065431</td>\n",
       "      <td>1.069949</td>\n",
       "      <td>0.827266</td>\n",
       "      <td>-0.006643</td>\n",
       "      <td>-7.881300</td>\n",
       "      <td>-1.019437</td>\n",
       "      <td>7.875589</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.0%</td>\n",
       "      <td>-2.040558</td>\n",
       "      <td>11.702136</td>\n",
       "      <td>4.870884</td>\n",
       "      <td>-1.429224</td>\n",
       "      <td>$-426.46</td>\n",
       "      <td>-6.335143</td>\n",
       "      <td>6.996686</td>\n",
       "      <td>24.878142</td>\n",
       "      <td>-7.006167</td>\n",
       "      <td>-2.841043</td>\n",
       "      <td>0.238350</td>\n",
       "      <td>3.655176</td>\n",
       "      <td>0.177542</td>\n",
       "      <td>-45.606018</td>\n",
       "      <td>-1.567650</td>\n",
       "      <td>-1.219998</td>\n",
       "      <td>-12.619720</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       x32       x33        x34       x35       x36        x37        x38  \\\n",
       "0     0.0% -1.940031  -5.492063  0.627121 -0.873824   $1313.96  -1.353729   \n",
       "1   -0.02%  2.211462  -4.460591  1.035461  0.228270   $1962.78  32.816804   \n",
       "2   -0.01%  0.419607  -3.804056 -0.763357 -1.612561    $430.47  -0.333199   \n",
       "3    0.01% -3.442715   4.420160  1.164532  3.033455  $-2366.29  14.188669   \n",
       "4    0.01% -0.431640  12.165494 -0.167726 -0.341604   $-620.66 -12.578926   \n",
       "5   -0.01%  0.962813   2.934661 -2.725477  0.656132   $-196.45  -6.555903   \n",
       "6   -0.01%  1.487652  19.546647 -4.279037 -0.000735   $-241.04 -14.102935   \n",
       "7    0.01%  0.467617  10.537118  0.061739  0.010240    $621.35  17.885994   \n",
       "8   -0.03% -0.704558  -9.100507  6.544267 -1.951658   $-301.89 -23.704439   \n",
       "9    0.01%  2.792562  -5.347096  1.084512  1.479771   $-484.09  17.421890   \n",
       "10   0.01% -3.280432   5.477774  4.298599  1.616987   $-106.65   4.546234   \n",
       "11   0.02%  2.213868   1.571707  1.239489  0.534812   $-622.76 -12.063685   \n",
       "12   0.01% -2.241071 -15.237309  0.629570  0.056979    $545.55  26.729089   \n",
       "13  -0.01% -0.522456 -12.537868 -0.473894 -0.931481   $1019.08  20.820001   \n",
       "14   -0.0% -0.708466  -6.669252 -0.330493  1.258759    $960.95  29.194515   \n",
       "15  -0.01% -1.238546  -5.213736  2.255307  2.445620    $299.52  23.654955   \n",
       "16  -0.01%  1.323644   2.647041 -1.229748  0.479819     $72.83   9.572061   \n",
       "17   -0.0%  2.908810  -9.367590 -4.144378 -1.585798    $-12.09 -14.349460   \n",
       "18    0.0% -2.902935  14.768572 -2.019176 -0.940080   $1074.27   7.066684   \n",
       "19   -0.0% -2.040558  11.702136  4.870884 -1.429224   $-426.46  -6.335143   \n",
       "\n",
       "          x39        x40        x41       x42       x43       x44       x45  \\\n",
       "0   -5.186148 -10.612200  -1.497117  5.414063 -2.325655  1.674827 -0.264332   \n",
       "1   -5.150012   2.147427  36.292790  4.490915  0.762561  6.526662  1.007927   \n",
       "2    8.728585  -0.863137  -0.368491  9.088864 -0.689886 -2.731118  0.754200   \n",
       "3   -6.385060  12.084421  15.691546 -7.467775  2.940789 -6.424112  0.419776   \n",
       "4    1.133798  30.004727 -13.911297 -5.229937  1.783928  3.957801 -0.096988   \n",
       "5    6.014648   3.091261  -7.250310  6.402343 -2.064860  0.923879  0.331452   \n",
       "6   -6.139318  -8.818327 -15.596731 -7.779833 -1.153986 -1.081095 -0.538983   \n",
       "7   11.162321  -2.479899  19.780494 -3.711105 -3.134008 -5.805255  0.966762   \n",
       "8    7.594268   7.576010 -26.215235  0.330604 -2.015631  2.219910  0.533815   \n",
       "9   -0.302757  -9.152182  19.267232 -0.675888 -2.560935  1.326146  0.444997   \n",
       "10  -2.775233  14.001419   5.027775 -3.903575  1.938380 -5.395413  0.459957   \n",
       "11  -2.632316   7.565319 -13.341481 -8.749303 -2.927836 -4.118254  0.790090   \n",
       "12   0.979735 -12.285052  29.560258 -5.199020 -2.492575  6.754616 -0.951078   \n",
       "13   6.666158 -11.567399  23.025274 -8.812971 -0.233724 -4.021603 -0.573178   \n",
       "14   5.363980  -8.897842  32.286825  0.724720  0.702446 -1.832473 -0.092212   \n",
       "15   4.497487 -52.283301  26.160509  0.290090 -0.984782  1.954579  0.383549   \n",
       "16  -7.938113 -12.132429  10.585942 -6.545866  1.088224 -2.185282 -0.230979   \n",
       "17  -3.660668  19.549106 -15.869368 -1.301406 -2.784866  3.139330 -0.330494   \n",
       "18  -0.915279   6.367554   7.815194  1.138352  0.065431  1.069949  0.827266   \n",
       "19   6.996686  24.878142  -7.006167 -2.841043  0.238350  3.655176  0.177542   \n",
       "\n",
       "          x46        x47       x48        x49  y  \n",
       "0   60.781427  -7.689696  0.151589  -8.040166  0  \n",
       "1   15.805696  -4.896678 -0.320283  16.719974  0  \n",
       "2   30.856417  -7.428573 -2.090804  -7.869421  0  \n",
       "3  -72.424569   5.361375  1.806070  -7.670847  0  \n",
       "4  -14.085435  -0.208351 -0.894942  15.724742  1  \n",
       "5   19.172365   5.752749 -2.609553 -20.320179  0  \n",
       "6    1.573406   2.102627 -2.966103   4.604337  1  \n",
       "7    6.639034   6.258786  1.272556  13.186184  1  \n",
       "8   -2.627660   0.182319  3.140230  14.495677  0  \n",
       "9  -20.942887  -3.141996 -1.608473   7.169219  1  \n",
       "10 -77.491333   0.754309 -0.442017 -21.824215  1  \n",
       "11 -95.290753  -2.076083  0.841078 -16.644318  0  \n",
       "12 -73.732913  -3.624559  1.414010 -25.409134  0  \n",
       "13   5.543237  -4.584788 -0.376251   3.194482  1  \n",
       "14   8.723096  -1.059007  2.602944  19.141027  0  \n",
       "15  -1.467533  13.830803 -2.831817   9.343167  0  \n",
       "16  21.335008  -1.517562 -0.445338   9.285682  0  \n",
       "17 -28.760882  -2.793687  0.801223  -2.128184  0  \n",
       "18  -0.006643  -7.881300 -1.019437   7.875589  0  \n",
       "19 -45.606018  -1.567650 -1.219998 -12.619720  0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.iloc[:,32:].head(n=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30893793",
   "metadata": {},
   "source": [
    "### Inspecting real data types \n",
    "\n",
    "- x24 = categorical\n",
    "- x29 = categorical\n",
    "- x30 = categorical\n",
    "- x32 = numerical but it has '%' char that needs to be removed.\n",
    "- x37 = numerical but it has '$' that needs to be removed.\n",
    "- y is the response variable but it needs to be converted to categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fd35aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have a binary response that we can convert to categorical\n",
    "df1['y'] = df1['y'].astype(\"category\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5035393",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=['x24', 'x29', 'x30']\n",
    "for col in df1:\n",
    "    if col in cols:\n",
    "    #if df1[col].dtypes in ['object']:\n",
    "        df1[col] = df1[col].astype(\"category\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f623e1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 160000 entries, 0 to 159999\n",
      "Data columns (total 51 columns):\n",
      " #   Column  Non-Null Count   Dtype   \n",
      "---  ------  --------------   -----   \n",
      " 0   x0      159974 non-null  float64 \n",
      " 1   x1      159975 non-null  float64 \n",
      " 2   x2      159962 non-null  float64 \n",
      " 3   x3      159963 non-null  float64 \n",
      " 4   x4      159974 non-null  float64 \n",
      " 5   x5      159963 non-null  float64 \n",
      " 6   x6      159974 non-null  float64 \n",
      " 7   x7      159973 non-null  float64 \n",
      " 8   x8      159979 non-null  float64 \n",
      " 9   x9      159970 non-null  float64 \n",
      " 10  x10     159957 non-null  float64 \n",
      " 11  x11     159970 non-null  float64 \n",
      " 12  x12     159964 non-null  float64 \n",
      " 13  x13     159969 non-null  float64 \n",
      " 14  x14     159966 non-null  float64 \n",
      " 15  x15     159965 non-null  float64 \n",
      " 16  x16     159974 non-null  float64 \n",
      " 17  x17     159973 non-null  float64 \n",
      " 18  x18     159960 non-null  float64 \n",
      " 19  x19     159965 non-null  float64 \n",
      " 20  x20     159962 non-null  float64 \n",
      " 21  x21     159971 non-null  float64 \n",
      " 22  x22     159973 non-null  float64 \n",
      " 23  x23     159953 non-null  float64 \n",
      " 24  x24     159972 non-null  category\n",
      " 25  x25     159978 non-null  float64 \n",
      " 26  x26     159964 non-null  float64 \n",
      " 27  x27     159970 non-null  float64 \n",
      " 28  x28     159965 non-null  float64 \n",
      " 29  x29     159970 non-null  category\n",
      " 30  x30     159970 non-null  category\n",
      " 31  x31     159961 non-null  float64 \n",
      " 32  x32     159969 non-null  object  \n",
      " 33  x33     159959 non-null  float64 \n",
      " 34  x34     159959 non-null  float64 \n",
      " 35  x35     159970 non-null  float64 \n",
      " 36  x36     159973 non-null  float64 \n",
      " 37  x37     159977 non-null  object  \n",
      " 38  x38     159969 non-null  float64 \n",
      " 39  x39     159977 non-null  float64 \n",
      " 40  x40     159964 non-null  float64 \n",
      " 41  x41     159960 non-null  float64 \n",
      " 42  x42     159974 non-null  float64 \n",
      " 43  x43     159963 non-null  float64 \n",
      " 44  x44     159960 non-null  float64 \n",
      " 45  x45     159971 non-null  float64 \n",
      " 46  x46     159969 non-null  float64 \n",
      " 47  x47     159963 non-null  float64 \n",
      " 48  x48     159968 non-null  float64 \n",
      " 49  x49     159968 non-null  float64 \n",
      " 50  y       160000 non-null  category\n",
      "dtypes: category(4), float64(45), object(2)\n",
      "memory usage: 58.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8728a72e",
   "metadata": {},
   "source": [
    "### Remove '%'  and '\\\\$' from affected columns.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9bbade3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['x32'] = df1['x32'].str.rstrip('%').astype('float64')\n",
    "df1['x37'] = df1['x37'].str.lstrip('$').astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0470c67b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x27</th>\n",
       "      <th>x28</th>\n",
       "      <th>x29</th>\n",
       "      <th>x30</th>\n",
       "      <th>x31</th>\n",
       "      <th>x32</th>\n",
       "      <th>x33</th>\n",
       "      <th>x34</th>\n",
       "      <th>x35</th>\n",
       "      <th>x36</th>\n",
       "      <th>x37</th>\n",
       "      <th>x38</th>\n",
       "      <th>x39</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.005131</td>\n",
       "      <td>-18.473784</td>\n",
       "      <td>July</td>\n",
       "      <td>tuesday</td>\n",
       "      <td>-3.851669</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-1.940031</td>\n",
       "      <td>-5.492063</td>\n",
       "      <td>0.627121</td>\n",
       "      <td>-0.873824</td>\n",
       "      <td>1313.96</td>\n",
       "      <td>-1.353729</td>\n",
       "      <td>-5.186148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.751086</td>\n",
       "      <td>3.749377</td>\n",
       "      <td>Aug</td>\n",
       "      <td>wednesday</td>\n",
       "      <td>1.391594</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>2.211462</td>\n",
       "      <td>-4.460591</td>\n",
       "      <td>1.035461</td>\n",
       "      <td>0.228270</td>\n",
       "      <td>1962.78</td>\n",
       "      <td>32.816804</td>\n",
       "      <td>-5.150012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.171088</td>\n",
       "      <td>11.522448</td>\n",
       "      <td>July</td>\n",
       "      <td>wednesday</td>\n",
       "      <td>-3.262082</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.419607</td>\n",
       "      <td>-3.804056</td>\n",
       "      <td>-0.763357</td>\n",
       "      <td>-1.612561</td>\n",
       "      <td>430.47</td>\n",
       "      <td>-0.333199</td>\n",
       "      <td>8.728585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.215569</td>\n",
       "      <td>30.595226</td>\n",
       "      <td>July</td>\n",
       "      <td>wednesday</td>\n",
       "      <td>-2.285241</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-3.442715</td>\n",
       "      <td>4.420160</td>\n",
       "      <td>1.164532</td>\n",
       "      <td>3.033455</td>\n",
       "      <td>-2366.29</td>\n",
       "      <td>14.188669</td>\n",
       "      <td>-6.385060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.811182</td>\n",
       "      <td>-4.094084</td>\n",
       "      <td>July</td>\n",
       "      <td>tuesday</td>\n",
       "      <td>0.921047</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.431640</td>\n",
       "      <td>12.165494</td>\n",
       "      <td>-0.167726</td>\n",
       "      <td>-0.341604</td>\n",
       "      <td>-620.66</td>\n",
       "      <td>-12.578926</td>\n",
       "      <td>1.133798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14.162735</td>\n",
       "      <td>5.567174</td>\n",
       "      <td>Aug</td>\n",
       "      <td>wednesday</td>\n",
       "      <td>1.168850</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.962813</td>\n",
       "      <td>2.934661</td>\n",
       "      <td>-2.725477</td>\n",
       "      <td>0.656132</td>\n",
       "      <td>-196.45</td>\n",
       "      <td>-6.555903</td>\n",
       "      <td>6.014648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-9.566602</td>\n",
       "      <td>9.618410</td>\n",
       "      <td>Jun</td>\n",
       "      <td>wednesday</td>\n",
       "      <td>-0.771455</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>1.487652</td>\n",
       "      <td>19.546647</td>\n",
       "      <td>-4.279037</td>\n",
       "      <td>-0.000735</td>\n",
       "      <td>-241.04</td>\n",
       "      <td>-14.102935</td>\n",
       "      <td>-6.139318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-4.372534</td>\n",
       "      <td>-5.655852</td>\n",
       "      <td>Aug</td>\n",
       "      <td>wednesday</td>\n",
       "      <td>-2.823480</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.467617</td>\n",
       "      <td>10.537118</td>\n",
       "      <td>0.061739</td>\n",
       "      <td>0.010240</td>\n",
       "      <td>621.35</td>\n",
       "      <td>17.885994</td>\n",
       "      <td>11.162321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-1.952634</td>\n",
       "      <td>19.140589</td>\n",
       "      <td>May</td>\n",
       "      <td>wednesday</td>\n",
       "      <td>-3.134230</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.704558</td>\n",
       "      <td>-9.100507</td>\n",
       "      <td>6.544267</td>\n",
       "      <td>-1.951658</td>\n",
       "      <td>-301.89</td>\n",
       "      <td>-23.704439</td>\n",
       "      <td>7.594268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3.231103</td>\n",
       "      <td>3.094954</td>\n",
       "      <td>Jun</td>\n",
       "      <td>wednesday</td>\n",
       "      <td>-2.150637</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2.792562</td>\n",
       "      <td>-5.347096</td>\n",
       "      <td>1.084512</td>\n",
       "      <td>1.479771</td>\n",
       "      <td>-484.09</td>\n",
       "      <td>17.421890</td>\n",
       "      <td>-0.302757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-7.120683</td>\n",
       "      <td>19.692815</td>\n",
       "      <td>July</td>\n",
       "      <td>wednesday</td>\n",
       "      <td>5.162031</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-3.280432</td>\n",
       "      <td>5.477774</td>\n",
       "      <td>4.298599</td>\n",
       "      <td>1.616987</td>\n",
       "      <td>-106.65</td>\n",
       "      <td>4.546234</td>\n",
       "      <td>-2.775233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-13.541087</td>\n",
       "      <td>-10.866080</td>\n",
       "      <td>July</td>\n",
       "      <td>tuesday</td>\n",
       "      <td>-0.002595</td>\n",
       "      <td>0.02</td>\n",
       "      <td>2.213868</td>\n",
       "      <td>1.571707</td>\n",
       "      <td>1.239489</td>\n",
       "      <td>0.534812</td>\n",
       "      <td>-622.76</td>\n",
       "      <td>-12.063685</td>\n",
       "      <td>-2.632316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-1.796590</td>\n",
       "      <td>-10.332773</td>\n",
       "      <td>Aug</td>\n",
       "      <td>thurday</td>\n",
       "      <td>0.650192</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-2.241071</td>\n",
       "      <td>-15.237309</td>\n",
       "      <td>0.629570</td>\n",
       "      <td>0.056979</td>\n",
       "      <td>545.55</td>\n",
       "      <td>26.729089</td>\n",
       "      <td>0.979735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.709254</td>\n",
       "      <td>-23.495351</td>\n",
       "      <td>May</td>\n",
       "      <td>tuesday</td>\n",
       "      <td>1.203463</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.522456</td>\n",
       "      <td>-12.537868</td>\n",
       "      <td>-0.473894</td>\n",
       "      <td>-0.931481</td>\n",
       "      <td>1019.08</td>\n",
       "      <td>20.820001</td>\n",
       "      <td>6.666158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-2.102940</td>\n",
       "      <td>6.314399</td>\n",
       "      <td>Jun</td>\n",
       "      <td>thurday</td>\n",
       "      <td>0.829312</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.708466</td>\n",
       "      <td>-6.669252</td>\n",
       "      <td>-0.330493</td>\n",
       "      <td>1.258759</td>\n",
       "      <td>960.95</td>\n",
       "      <td>29.194515</td>\n",
       "      <td>5.363980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-8.101993</td>\n",
       "      <td>13.199037</td>\n",
       "      <td>sept.</td>\n",
       "      <td>wednesday</td>\n",
       "      <td>-2.068144</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-1.238546</td>\n",
       "      <td>-5.213736</td>\n",
       "      <td>2.255307</td>\n",
       "      <td>2.445620</td>\n",
       "      <td>299.52</td>\n",
       "      <td>23.654955</td>\n",
       "      <td>4.497487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-1.961302</td>\n",
       "      <td>4.662057</td>\n",
       "      <td>sept.</td>\n",
       "      <td>wednesday</td>\n",
       "      <td>-3.866860</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>1.323644</td>\n",
       "      <td>2.647041</td>\n",
       "      <td>-1.229748</td>\n",
       "      <td>0.479819</td>\n",
       "      <td>72.83</td>\n",
       "      <td>9.572061</td>\n",
       "      <td>-7.938113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-5.906723</td>\n",
       "      <td>4.658615</td>\n",
       "      <td>Jun</td>\n",
       "      <td>thurday</td>\n",
       "      <td>0.255524</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>2.908810</td>\n",
       "      <td>-9.367590</td>\n",
       "      <td>-4.144378</td>\n",
       "      <td>-1.585798</td>\n",
       "      <td>-12.09</td>\n",
       "      <td>-14.349460</td>\n",
       "      <td>-3.660668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-10.359421</td>\n",
       "      <td>1.321779</td>\n",
       "      <td>Jun</td>\n",
       "      <td>wednesday</td>\n",
       "      <td>-0.216986</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-2.902935</td>\n",
       "      <td>14.768572</td>\n",
       "      <td>-2.019176</td>\n",
       "      <td>-0.940080</td>\n",
       "      <td>1074.27</td>\n",
       "      <td>7.066684</td>\n",
       "      <td>-0.915279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.133579</td>\n",
       "      <td>12.028623</td>\n",
       "      <td>May</td>\n",
       "      <td>tuesday</td>\n",
       "      <td>1.218476</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-2.040558</td>\n",
       "      <td>11.702136</td>\n",
       "      <td>4.870884</td>\n",
       "      <td>-1.429224</td>\n",
       "      <td>-426.46</td>\n",
       "      <td>-6.335143</td>\n",
       "      <td>6.996686</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          x27        x28    x29        x30       x31   x32       x33  \\\n",
       "0    1.005131 -18.473784   July    tuesday -3.851669  0.00 -1.940031   \n",
       "1    0.751086   3.749377    Aug  wednesday  1.391594 -0.02  2.211462   \n",
       "2    4.171088  11.522448   July  wednesday -3.262082 -0.01  0.419607   \n",
       "3    9.215569  30.595226   July  wednesday -2.285241  0.01 -3.442715   \n",
       "4    1.811182  -4.094084   July    tuesday  0.921047  0.01 -0.431640   \n",
       "5   14.162735   5.567174    Aug  wednesday  1.168850 -0.01  0.962813   \n",
       "6   -9.566602   9.618410    Jun  wednesday -0.771455 -0.01  1.487652   \n",
       "7   -4.372534  -5.655852    Aug  wednesday -2.823480  0.01  0.467617   \n",
       "8   -1.952634  19.140589    May  wednesday -3.134230 -0.03 -0.704558   \n",
       "9    3.231103   3.094954    Jun  wednesday -2.150637  0.01  2.792562   \n",
       "10  -7.120683  19.692815   July  wednesday  5.162031  0.01 -3.280432   \n",
       "11 -13.541087 -10.866080   July    tuesday -0.002595  0.02  2.213868   \n",
       "12  -1.796590 -10.332773    Aug    thurday  0.650192  0.01 -2.241071   \n",
       "13   0.709254 -23.495351    May    tuesday  1.203463 -0.01 -0.522456   \n",
       "14  -2.102940   6.314399    Jun    thurday  0.829312 -0.00 -0.708466   \n",
       "15  -8.101993  13.199037  sept.  wednesday -2.068144 -0.01 -1.238546   \n",
       "16  -1.961302   4.662057  sept.  wednesday -3.866860 -0.01  1.323644   \n",
       "17  -5.906723   4.658615    Jun    thurday  0.255524 -0.00  2.908810   \n",
       "18 -10.359421   1.321779    Jun  wednesday -0.216986  0.00 -2.902935   \n",
       "19   1.133579  12.028623    May    tuesday  1.218476 -0.00 -2.040558   \n",
       "\n",
       "          x34       x35       x36      x37        x38        x39  \n",
       "0   -5.492063  0.627121 -0.873824  1313.96  -1.353729  -5.186148  \n",
       "1   -4.460591  1.035461  0.228270  1962.78  32.816804  -5.150012  \n",
       "2   -3.804056 -0.763357 -1.612561   430.47  -0.333199   8.728585  \n",
       "3    4.420160  1.164532  3.033455 -2366.29  14.188669  -6.385060  \n",
       "4   12.165494 -0.167726 -0.341604  -620.66 -12.578926   1.133798  \n",
       "5    2.934661 -2.725477  0.656132  -196.45  -6.555903   6.014648  \n",
       "6   19.546647 -4.279037 -0.000735  -241.04 -14.102935  -6.139318  \n",
       "7   10.537118  0.061739  0.010240   621.35  17.885994  11.162321  \n",
       "8   -9.100507  6.544267 -1.951658  -301.89 -23.704439   7.594268  \n",
       "9   -5.347096  1.084512  1.479771  -484.09  17.421890  -0.302757  \n",
       "10   5.477774  4.298599  1.616987  -106.65   4.546234  -2.775233  \n",
       "11   1.571707  1.239489  0.534812  -622.76 -12.063685  -2.632316  \n",
       "12 -15.237309  0.629570  0.056979   545.55  26.729089   0.979735  \n",
       "13 -12.537868 -0.473894 -0.931481  1019.08  20.820001   6.666158  \n",
       "14  -6.669252 -0.330493  1.258759   960.95  29.194515   5.363980  \n",
       "15  -5.213736  2.255307  2.445620   299.52  23.654955   4.497487  \n",
       "16   2.647041 -1.229748  0.479819    72.83   9.572061  -7.938113  \n",
       "17  -9.367590 -4.144378 -1.585798   -12.09 -14.349460  -3.660668  \n",
       "18  14.768572 -2.019176 -0.940080  1074.27   7.066684  -0.915279  \n",
       "19  11.702136  4.870884 -1.429224  -426.46  -6.335143   6.996686  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.iloc[:,27:40].head(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da898bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 160000 entries, 0 to 159999\n",
      "Data columns (total 51 columns):\n",
      " #   Column  Non-Null Count   Dtype   \n",
      "---  ------  --------------   -----   \n",
      " 0   x0      159974 non-null  float64 \n",
      " 1   x1      159975 non-null  float64 \n",
      " 2   x2      159962 non-null  float64 \n",
      " 3   x3      159963 non-null  float64 \n",
      " 4   x4      159974 non-null  float64 \n",
      " 5   x5      159963 non-null  float64 \n",
      " 6   x6      159974 non-null  float64 \n",
      " 7   x7      159973 non-null  float64 \n",
      " 8   x8      159979 non-null  float64 \n",
      " 9   x9      159970 non-null  float64 \n",
      " 10  x10     159957 non-null  float64 \n",
      " 11  x11     159970 non-null  float64 \n",
      " 12  x12     159964 non-null  float64 \n",
      " 13  x13     159969 non-null  float64 \n",
      " 14  x14     159966 non-null  float64 \n",
      " 15  x15     159965 non-null  float64 \n",
      " 16  x16     159974 non-null  float64 \n",
      " 17  x17     159973 non-null  float64 \n",
      " 18  x18     159960 non-null  float64 \n",
      " 19  x19     159965 non-null  float64 \n",
      " 20  x20     159962 non-null  float64 \n",
      " 21  x21     159971 non-null  float64 \n",
      " 22  x22     159973 non-null  float64 \n",
      " 23  x23     159953 non-null  float64 \n",
      " 24  x24     159972 non-null  category\n",
      " 25  x25     159978 non-null  float64 \n",
      " 26  x26     159964 non-null  float64 \n",
      " 27  x27     159970 non-null  float64 \n",
      " 28  x28     159965 non-null  float64 \n",
      " 29  x29     159970 non-null  category\n",
      " 30  x30     159970 non-null  category\n",
      " 31  x31     159961 non-null  float64 \n",
      " 32  x32     159969 non-null  float64 \n",
      " 33  x33     159959 non-null  float64 \n",
      " 34  x34     159959 non-null  float64 \n",
      " 35  x35     159970 non-null  float64 \n",
      " 36  x36     159973 non-null  float64 \n",
      " 37  x37     159977 non-null  float64 \n",
      " 38  x38     159969 non-null  float64 \n",
      " 39  x39     159977 non-null  float64 \n",
      " 40  x40     159964 non-null  float64 \n",
      " 41  x41     159960 non-null  float64 \n",
      " 42  x42     159974 non-null  float64 \n",
      " 43  x43     159963 non-null  float64 \n",
      " 44  x44     159960 non-null  float64 \n",
      " 45  x45     159971 non-null  float64 \n",
      " 46  x46     159969 non-null  float64 \n",
      " 47  x47     159963 non-null  float64 \n",
      " 48  x48     159968 non-null  float64 \n",
      " 49  x49     159968 non-null  float64 \n",
      " 50  y       160000 non-null  category\n",
      "dtypes: category(4), float64(47)\n",
      "memory usage: 58.0 MB\n"
     ]
    }
   ],
   "source": [
    "df1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14dfe4f7",
   "metadata": {},
   "source": [
    "**The dataset contains 50 attributes: 4 categorical and 46 numerical represented as float.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24c9917",
   "metadata": {},
   "source": [
    "## check for duplicated rows.  \n",
    "- None found in this dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96819ff7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>...</th>\n",
       "      <th>x41</th>\n",
       "      <th>x42</th>\n",
       "      <th>x43</th>\n",
       "      <th>x44</th>\n",
       "      <th>x45</th>\n",
       "      <th>x46</th>\n",
       "      <th>x47</th>\n",
       "      <th>x48</th>\n",
       "      <th>x49</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [x0, x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12, x13, x14, x15, x16, x17, x18, x19, x20, x21, x22, x23, x24, x25, x26, x27, x28, x29, x30, x31, x32, x33, x34, x35, x36, x37, x38, x39, x40, x41, x42, x43, x44, x45, x46, x47, x48, x49, y]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 51 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicate = df1[df1.duplicated()]\n",
    "duplicate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d59472",
   "metadata": {},
   "source": [
    "## Missing values. \n",
    "The missing values are much less than 1% of the entire dataset.  \n",
    "So we can drop them.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "abf5db16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bca57e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 158392 entries, 0 to 159999\n",
      "Data columns (total 51 columns):\n",
      " #   Column  Non-Null Count   Dtype   \n",
      "---  ------  --------------   -----   \n",
      " 0   x0      158392 non-null  float64 \n",
      " 1   x1      158392 non-null  float64 \n",
      " 2   x2      158392 non-null  float64 \n",
      " 3   x3      158392 non-null  float64 \n",
      " 4   x4      158392 non-null  float64 \n",
      " 5   x5      158392 non-null  float64 \n",
      " 6   x6      158392 non-null  float64 \n",
      " 7   x7      158392 non-null  float64 \n",
      " 8   x8      158392 non-null  float64 \n",
      " 9   x9      158392 non-null  float64 \n",
      " 10  x10     158392 non-null  float64 \n",
      " 11  x11     158392 non-null  float64 \n",
      " 12  x12     158392 non-null  float64 \n",
      " 13  x13     158392 non-null  float64 \n",
      " 14  x14     158392 non-null  float64 \n",
      " 15  x15     158392 non-null  float64 \n",
      " 16  x16     158392 non-null  float64 \n",
      " 17  x17     158392 non-null  float64 \n",
      " 18  x18     158392 non-null  float64 \n",
      " 19  x19     158392 non-null  float64 \n",
      " 20  x20     158392 non-null  float64 \n",
      " 21  x21     158392 non-null  float64 \n",
      " 22  x22     158392 non-null  float64 \n",
      " 23  x23     158392 non-null  float64 \n",
      " 24  x24     158392 non-null  category\n",
      " 25  x25     158392 non-null  float64 \n",
      " 26  x26     158392 non-null  float64 \n",
      " 27  x27     158392 non-null  float64 \n",
      " 28  x28     158392 non-null  float64 \n",
      " 29  x29     158392 non-null  category\n",
      " 30  x30     158392 non-null  category\n",
      " 31  x31     158392 non-null  float64 \n",
      " 32  x32     158392 non-null  float64 \n",
      " 33  x33     158392 non-null  float64 \n",
      " 34  x34     158392 non-null  float64 \n",
      " 35  x35     158392 non-null  float64 \n",
      " 36  x36     158392 non-null  float64 \n",
      " 37  x37     158392 non-null  float64 \n",
      " 38  x38     158392 non-null  float64 \n",
      " 39  x39     158392 non-null  float64 \n",
      " 40  x40     158392 non-null  float64 \n",
      " 41  x41     158392 non-null  float64 \n",
      " 42  x42     158392 non-null  float64 \n",
      " 43  x43     158392 non-null  float64 \n",
      " 44  x44     158392 non-null  float64 \n",
      " 45  x45     158392 non-null  float64 \n",
      " 46  x46     158392 non-null  float64 \n",
      " 47  x47     158392 non-null  float64 \n",
      " 48  x48     158392 non-null  float64 \n",
      " 49  x49     158392 non-null  float64 \n",
      " 50  y       158392 non-null  category\n",
      "dtypes: category(4), float64(47)\n",
      "memory usage: 58.6 MB\n"
     ]
    }
   ],
   "source": [
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b2ad9863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.010050000000000003"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we lost ~1% to dropping the missung values.\n",
    "# this is a'big' dataset so the impact of the removing the missing NAs will be minimal.\n",
    "1 - (158392/160000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a067587e",
   "metadata": {},
   "source": [
    "## Class imbalance.  \n",
    "- We need to check for class imbalance.  \n",
    "    - As we can see below, class imbalance is not a huge problem given a ~60/40 ratio in distribution with respect to the target variable.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c12e05c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.598805\n",
       "1    0.401195\n",
       "Name: y, dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['y'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6884029",
   "metadata": {},
   "source": [
    "## Outlier  \n",
    "- Checking for significant oultier to see if I need to standardize any features. \n",
    "    - The box method of the matplotlib library will be used to make univariate plots.  \n",
    "    - There is no significant outlier in any of the dataset.  \n",
    "    - But we will still use robust scaler to standardize our dataset.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f737e6f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "df1.iloc[:,38:].plot(kind='box')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "df946dd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "df1.iloc[:,:20].plot(kind='box')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4b03a6f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "df1.iloc[:,20:36].plot(kind='box')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae74be0",
   "metadata": {},
   "source": [
    "### Data distribution.  \n",
    "Looking the boxplots, it is clear that all the attributes do not contain very extreme outliers and are normally distributed.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f792ea82",
   "metadata": {},
   "source": [
    "## Onehotencoding  for features.  \n",
    "- We will encode the categorical features as 0,1.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7e7053bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(158392,)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Remove the target response from the predictor attributes. \n",
    "#Ensure the shape is the same as that of the main dataset.  \n",
    "y = df1.y\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d98d5b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.preprocessing import RobustScaler\n",
    "#import numpy as np\n",
    "df2 =copy.deepcopy(df1) \n",
    "for col in df1:\n",
    "    if df1[col].dtype.name in ['category']:\n",
    "        tmp_df = pd.get_dummies(df1[col], prefix=col) #Onehot encoding.  \n",
    "        df2 = pd.concat((df2,tmp_df),axis=1)\n",
    "        #df1[col]= trans.fit_transform(df[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ded59022",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 158392 entries, 0 to 159999\n",
      "Data columns (total 73 columns):\n",
      " #   Column         Non-Null Count   Dtype   \n",
      "---  ------         --------------   -----   \n",
      " 0   x0             158392 non-null  float64 \n",
      " 1   x1             158392 non-null  float64 \n",
      " 2   x2             158392 non-null  float64 \n",
      " 3   x3             158392 non-null  float64 \n",
      " 4   x4             158392 non-null  float64 \n",
      " 5   x5             158392 non-null  float64 \n",
      " 6   x6             158392 non-null  float64 \n",
      " 7   x7             158392 non-null  float64 \n",
      " 8   x8             158392 non-null  float64 \n",
      " 9   x9             158392 non-null  float64 \n",
      " 10  x10            158392 non-null  float64 \n",
      " 11  x11            158392 non-null  float64 \n",
      " 12  x12            158392 non-null  float64 \n",
      " 13  x13            158392 non-null  float64 \n",
      " 14  x14            158392 non-null  float64 \n",
      " 15  x15            158392 non-null  float64 \n",
      " 16  x16            158392 non-null  float64 \n",
      " 17  x17            158392 non-null  float64 \n",
      " 18  x18            158392 non-null  float64 \n",
      " 19  x19            158392 non-null  float64 \n",
      " 20  x20            158392 non-null  float64 \n",
      " 21  x21            158392 non-null  float64 \n",
      " 22  x22            158392 non-null  float64 \n",
      " 23  x23            158392 non-null  float64 \n",
      " 24  x24            158392 non-null  category\n",
      " 25  x25            158392 non-null  float64 \n",
      " 26  x26            158392 non-null  float64 \n",
      " 27  x27            158392 non-null  float64 \n",
      " 28  x28            158392 non-null  float64 \n",
      " 29  x29            158392 non-null  category\n",
      " 30  x30            158392 non-null  category\n",
      " 31  x31            158392 non-null  float64 \n",
      " 32  x32            158392 non-null  float64 \n",
      " 33  x33            158392 non-null  float64 \n",
      " 34  x34            158392 non-null  float64 \n",
      " 35  x35            158392 non-null  float64 \n",
      " 36  x36            158392 non-null  float64 \n",
      " 37  x37            158392 non-null  float64 \n",
      " 38  x38            158392 non-null  float64 \n",
      " 39  x39            158392 non-null  float64 \n",
      " 40  x40            158392 non-null  float64 \n",
      " 41  x41            158392 non-null  float64 \n",
      " 42  x42            158392 non-null  float64 \n",
      " 43  x43            158392 non-null  float64 \n",
      " 44  x44            158392 non-null  float64 \n",
      " 45  x45            158392 non-null  float64 \n",
      " 46  x46            158392 non-null  float64 \n",
      " 47  x47            158392 non-null  float64 \n",
      " 48  x48            158392 non-null  float64 \n",
      " 49  x49            158392 non-null  float64 \n",
      " 50  y              158392 non-null  category\n",
      " 51  x24_america    158392 non-null  uint8   \n",
      " 52  x24_asia       158392 non-null  uint8   \n",
      " 53  x24_euorpe     158392 non-null  uint8   \n",
      " 54  x29_Apr        158392 non-null  uint8   \n",
      " 55  x29_Aug        158392 non-null  uint8   \n",
      " 56  x29_Dev        158392 non-null  uint8   \n",
      " 57  x29_Feb        158392 non-null  uint8   \n",
      " 58  x29_January    158392 non-null  uint8   \n",
      " 59  x29_July       158392 non-null  uint8   \n",
      " 60  x29_Jun        158392 non-null  uint8   \n",
      " 61  x29_Mar        158392 non-null  uint8   \n",
      " 62  x29_May        158392 non-null  uint8   \n",
      " 63  x29_Nov        158392 non-null  uint8   \n",
      " 64  x29_Oct        158392 non-null  uint8   \n",
      " 65  x29_sept.      158392 non-null  uint8   \n",
      " 66  x30_friday     158392 non-null  uint8   \n",
      " 67  x30_monday     158392 non-null  uint8   \n",
      " 68  x30_thurday    158392 non-null  uint8   \n",
      " 69  x30_tuesday    158392 non-null  uint8   \n",
      " 70  x30_wednesday  158392 non-null  uint8   \n",
      " 71  y_0            158392 non-null  uint8   \n",
      " 72  y_1            158392 non-null  uint8   \n",
      "dtypes: category(4), float64(47), uint8(22)\n",
      "memory usage: 61.9 MB\n"
     ]
    }
   ],
   "source": [
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7cf95d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop the encoded variables. \n",
    "df2.drop(columns=['x24', 'x29', 'x30', 'y', 'y_0','y_1'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5f767080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 158392 entries, 0 to 159999\n",
      "Data columns (total 67 columns):\n",
      " #   Column         Non-Null Count   Dtype  \n",
      "---  ------         --------------   -----  \n",
      " 0   x0             158392 non-null  float64\n",
      " 1   x1             158392 non-null  float64\n",
      " 2   x2             158392 non-null  float64\n",
      " 3   x3             158392 non-null  float64\n",
      " 4   x4             158392 non-null  float64\n",
      " 5   x5             158392 non-null  float64\n",
      " 6   x6             158392 non-null  float64\n",
      " 7   x7             158392 non-null  float64\n",
      " 8   x8             158392 non-null  float64\n",
      " 9   x9             158392 non-null  float64\n",
      " 10  x10            158392 non-null  float64\n",
      " 11  x11            158392 non-null  float64\n",
      " 12  x12            158392 non-null  float64\n",
      " 13  x13            158392 non-null  float64\n",
      " 14  x14            158392 non-null  float64\n",
      " 15  x15            158392 non-null  float64\n",
      " 16  x16            158392 non-null  float64\n",
      " 17  x17            158392 non-null  float64\n",
      " 18  x18            158392 non-null  float64\n",
      " 19  x19            158392 non-null  float64\n",
      " 20  x20            158392 non-null  float64\n",
      " 21  x21            158392 non-null  float64\n",
      " 22  x22            158392 non-null  float64\n",
      " 23  x23            158392 non-null  float64\n",
      " 24  x25            158392 non-null  float64\n",
      " 25  x26            158392 non-null  float64\n",
      " 26  x27            158392 non-null  float64\n",
      " 27  x28            158392 non-null  float64\n",
      " 28  x31            158392 non-null  float64\n",
      " 29  x32            158392 non-null  float64\n",
      " 30  x33            158392 non-null  float64\n",
      " 31  x34            158392 non-null  float64\n",
      " 32  x35            158392 non-null  float64\n",
      " 33  x36            158392 non-null  float64\n",
      " 34  x37            158392 non-null  float64\n",
      " 35  x38            158392 non-null  float64\n",
      " 36  x39            158392 non-null  float64\n",
      " 37  x40            158392 non-null  float64\n",
      " 38  x41            158392 non-null  float64\n",
      " 39  x42            158392 non-null  float64\n",
      " 40  x43            158392 non-null  float64\n",
      " 41  x44            158392 non-null  float64\n",
      " 42  x45            158392 non-null  float64\n",
      " 43  x46            158392 non-null  float64\n",
      " 44  x47            158392 non-null  float64\n",
      " 45  x48            158392 non-null  float64\n",
      " 46  x49            158392 non-null  float64\n",
      " 47  x24_america    158392 non-null  uint8  \n",
      " 48  x24_asia       158392 non-null  uint8  \n",
      " 49  x24_euorpe     158392 non-null  uint8  \n",
      " 50  x29_Apr        158392 non-null  uint8  \n",
      " 51  x29_Aug        158392 non-null  uint8  \n",
      " 52  x29_Dev        158392 non-null  uint8  \n",
      " 53  x29_Feb        158392 non-null  uint8  \n",
      " 54  x29_January    158392 non-null  uint8  \n",
      " 55  x29_July       158392 non-null  uint8  \n",
      " 56  x29_Jun        158392 non-null  uint8  \n",
      " 57  x29_Mar        158392 non-null  uint8  \n",
      " 58  x29_May        158392 non-null  uint8  \n",
      " 59  x29_Nov        158392 non-null  uint8  \n",
      " 60  x29_Oct        158392 non-null  uint8  \n",
      " 61  x29_sept.      158392 non-null  uint8  \n",
      " 62  x30_friday     158392 non-null  uint8  \n",
      " 63  x30_monday     158392 non-null  uint8  \n",
      " 64  x30_thurday    158392 non-null  uint8  \n",
      " 65  x30_tuesday    158392 non-null  uint8  \n",
      " 66  x30_wednesday  158392 non-null  uint8  \n",
      "dtypes: float64(47), uint8(20)\n",
      "memory usage: 61.0 MB\n"
     ]
    }
   ],
   "source": [
    "df2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78efd4ea",
   "metadata": {},
   "source": [
    "## Train-test split.  \n",
    "- The new dataset now has 67 columns after the onehotencoding.  \n",
    "- We will kep 15% as test set to evaluate our model.  \n",
    "    - It is not essential to stratify sampling because the classes are nearly balanced. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "63f91d7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test, y_train, y_test = train_test_split(df2, y, test_size=0.15,random_state=42, stratify = y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "38171198",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbafe19",
   "metadata": {},
   "source": [
    "## Normalize, Scale.  \n",
    "- The idea is to normalize data that are not normally distributed and standardize/scale data so that all the attributes have a mean = 0 and standard deviation = 1. \n",
    "- However, we do not want to accidentally use the testing data to determine the mean and std (this would be snooping).  \n",
    "- I found this analogy on stackexchange: \n",
    "    - https://datascience.stackexchange.com/questions/38395/standardscaler-before-and-after-splitting-data    \n",
    "    - https://stackoverflow.com/questions/63037248/is-it-correct-to-use-a-single-standardscaler-before-splitting-data    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e22cbac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train =  scaler.fit_transform(train)\n",
    "X_test = scaler.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "28670e69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>...</th>\n",
       "      <th>x29_Mar</th>\n",
       "      <th>x29_May</th>\n",
       "      <th>x29_Nov</th>\n",
       "      <th>x29_Oct</th>\n",
       "      <th>x29_sept.</th>\n",
       "      <th>x30_friday</th>\n",
       "      <th>x30_monday</th>\n",
       "      <th>x30_thurday</th>\n",
       "      <th>x30_tuesday</th>\n",
       "      <th>x30_wednesday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>102251</th>\n",
       "      <td>-0.092518</td>\n",
       "      <td>0.065540</td>\n",
       "      <td>-8.106073</td>\n",
       "      <td>-5.063874</td>\n",
       "      <td>10.995301</td>\n",
       "      <td>-0.831157</td>\n",
       "      <td>-11.785621</td>\n",
       "      <td>-44.656776</td>\n",
       "      <td>6.018197</td>\n",
       "      <td>-5.250462</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144768</th>\n",
       "      <td>-0.144375</td>\n",
       "      <td>4.481236</td>\n",
       "      <td>13.274440</td>\n",
       "      <td>-13.313492</td>\n",
       "      <td>-11.828845</td>\n",
       "      <td>-3.760655</td>\n",
       "      <td>19.300038</td>\n",
       "      <td>-49.196581</td>\n",
       "      <td>-1.577703</td>\n",
       "      <td>-1.330635</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28775</th>\n",
       "      <td>-0.055396</td>\n",
       "      <td>0.710109</td>\n",
       "      <td>-9.349981</td>\n",
       "      <td>1.199072</td>\n",
       "      <td>7.635552</td>\n",
       "      <td>7.682742</td>\n",
       "      <td>-13.594170</td>\n",
       "      <td>37.831414</td>\n",
       "      <td>-8.713023</td>\n",
       "      <td>-0.987806</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38543</th>\n",
       "      <td>0.177142</td>\n",
       "      <td>-0.425440</td>\n",
       "      <td>10.100688</td>\n",
       "      <td>12.389237</td>\n",
       "      <td>-2.125743</td>\n",
       "      <td>-7.147561</td>\n",
       "      <td>14.685641</td>\n",
       "      <td>-9.179316</td>\n",
       "      <td>0.808538</td>\n",
       "      <td>2.184415</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573</th>\n",
       "      <td>0.272088</td>\n",
       "      <td>2.908309</td>\n",
       "      <td>-18.353995</td>\n",
       "      <td>-0.947516</td>\n",
       "      <td>8.802744</td>\n",
       "      <td>0.016342</td>\n",
       "      <td>-26.685329</td>\n",
       "      <td>-33.253244</td>\n",
       "      <td>-7.452920</td>\n",
       "      <td>11.121937</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58335</th>\n",
       "      <td>0.161087</td>\n",
       "      <td>-12.813000</td>\n",
       "      <td>-14.617766</td>\n",
       "      <td>1.134675</td>\n",
       "      <td>-3.766302</td>\n",
       "      <td>-7.813302</td>\n",
       "      <td>-21.253134</td>\n",
       "      <td>7.482838</td>\n",
       "      <td>-1.000690</td>\n",
       "      <td>-4.073181</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151696</th>\n",
       "      <td>-0.136296</td>\n",
       "      <td>2.995620</td>\n",
       "      <td>25.755322</td>\n",
       "      <td>-2.776815</td>\n",
       "      <td>-3.862744</td>\n",
       "      <td>10.969033</td>\n",
       "      <td>37.446302</td>\n",
       "      <td>-58.721994</td>\n",
       "      <td>13.827378</td>\n",
       "      <td>3.135855</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119184</th>\n",
       "      <td>0.357479</td>\n",
       "      <td>9.463297</td>\n",
       "      <td>9.590006</td>\n",
       "      <td>3.356048</td>\n",
       "      <td>-2.428666</td>\n",
       "      <td>0.000246</td>\n",
       "      <td>13.943149</td>\n",
       "      <td>1.351641</td>\n",
       "      <td>12.627159</td>\n",
       "      <td>2.894304</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16311</th>\n",
       "      <td>0.273437</td>\n",
       "      <td>9.988583</td>\n",
       "      <td>-11.006676</td>\n",
       "      <td>5.655793</td>\n",
       "      <td>10.364243</td>\n",
       "      <td>3.684702</td>\n",
       "      <td>-16.002879</td>\n",
       "      <td>11.025653</td>\n",
       "      <td>-2.406340</td>\n",
       "      <td>2.051181</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13177</th>\n",
       "      <td>-0.939810</td>\n",
       "      <td>-0.553001</td>\n",
       "      <td>-26.844265</td>\n",
       "      <td>-1.727424</td>\n",
       "      <td>-4.619575</td>\n",
       "      <td>9.563396</td>\n",
       "      <td>-39.029545</td>\n",
       "      <td>-41.822414</td>\n",
       "      <td>1.735833</td>\n",
       "      <td>5.099046</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>134633 rows × 67 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              x0         x1         x2         x3         x4         x5  \\\n",
       "102251 -0.092518   0.065540  -8.106073  -5.063874  10.995301  -0.831157   \n",
       "144768 -0.144375   4.481236  13.274440 -13.313492 -11.828845  -3.760655   \n",
       "28775  -0.055396   0.710109  -9.349981   1.199072   7.635552   7.682742   \n",
       "38543   0.177142  -0.425440  10.100688  12.389237  -2.125743  -7.147561   \n",
       "573     0.272088   2.908309 -18.353995  -0.947516   8.802744   0.016342   \n",
       "...          ...        ...        ...        ...        ...        ...   \n",
       "58335   0.161087 -12.813000 -14.617766   1.134675  -3.766302  -7.813302   \n",
       "151696 -0.136296   2.995620  25.755322  -2.776815  -3.862744  10.969033   \n",
       "119184  0.357479   9.463297   9.590006   3.356048  -2.428666   0.000246   \n",
       "16311   0.273437   9.988583 -11.006676   5.655793  10.364243   3.684702   \n",
       "13177  -0.939810  -0.553001 -26.844265  -1.727424  -4.619575   9.563396   \n",
       "\n",
       "               x6         x7         x8         x9  ...  x29_Mar  x29_May  \\\n",
       "102251 -11.785621 -44.656776   6.018197  -5.250462  ...        0        0   \n",
       "144768  19.300038 -49.196581  -1.577703  -1.330635  ...        0        0   \n",
       "28775  -13.594170  37.831414  -8.713023  -0.987806  ...        0        0   \n",
       "38543   14.685641  -9.179316   0.808538   2.184415  ...        0        0   \n",
       "573    -26.685329 -33.253244  -7.452920  11.121937  ...        0        0   \n",
       "...           ...        ...        ...        ...  ...      ...      ...   \n",
       "58335  -21.253134   7.482838  -1.000690  -4.073181  ...        0        0   \n",
       "151696  37.446302 -58.721994  13.827378   3.135855  ...        0        0   \n",
       "119184  13.943149   1.351641  12.627159   2.894304  ...        0        0   \n",
       "16311  -16.002879  11.025653  -2.406340   2.051181  ...        0        0   \n",
       "13177  -39.029545 -41.822414   1.735833   5.099046  ...        0        0   \n",
       "\n",
       "        x29_Nov  x29_Oct  x29_sept.  x30_friday  x30_monday  x30_thurday  \\\n",
       "102251        0        0          0           0           0            0   \n",
       "144768        0        0          0           0           0            0   \n",
       "28775         0        0          0           0           0            1   \n",
       "38543         0        0          0           1           0            0   \n",
       "573           0        0          0           0           0            0   \n",
       "...         ...      ...        ...         ...         ...          ...   \n",
       "58335         0        0          0           0           0            0   \n",
       "151696        0        0          0           0           0            0   \n",
       "119184        0        0          0           0           0            0   \n",
       "16311         0        0          0           0           0            0   \n",
       "13177         0        0          0           0           0            0   \n",
       "\n",
       "        x30_tuesday  x30_wednesday  \n",
       "102251            0              1  \n",
       "144768            0              1  \n",
       "28775             0              0  \n",
       "38543             0              0  \n",
       "573               0              1  \n",
       "...             ...            ...  \n",
       "58335             0              1  \n",
       "151696            0              1  \n",
       "119184            0              1  \n",
       "16311             1              0  \n",
       "13177             0              1  \n",
       "\n",
       "[134633 rows x 67 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1504ae94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(134633, 67)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b24830f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.24719257,  0.00819287, -0.52295327, ..., -0.47455887,\n",
       "        -0.46060439,  0.75880117],\n",
       "       [-0.38685367,  0.70486456,  1.08708285, ..., -0.47455887,\n",
       "        -0.46060439,  0.75880117],\n",
       "       [-0.14721659,  0.10988759, -0.61662441, ...,  2.10722011,\n",
       "        -0.46060439, -1.31786829],\n",
       "       ...,\n",
       "       [ 0.96473247,  1.49089248,  0.80963062, ..., -0.47455887,\n",
       "        -0.46060439,  0.75880117],\n",
       "       [ 0.73839317,  1.57376776, -0.74137995, ..., -0.47455887,\n",
       "         2.17106049, -1.31786829],\n",
       "       [-2.52910964, -0.08939534, -1.9340123 , ..., -0.47455887,\n",
       "        -0.46060439,  0.75880117]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10647e15",
   "metadata": {},
   "source": [
    "## The feature names\n",
    "- Feature names are lost after applying the standard scaler.  \n",
    "- For better explainability, we retrieve them back.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "376ed4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scaled_features = pd.DataFrame(X_train, index=train.index, columns=train.columns)\n",
    "test_scaled_features = pd.DataFrame(X_test, index=test.index, columns=test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c57c7f1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>...</th>\n",
       "      <th>x29_Mar</th>\n",
       "      <th>x29_May</th>\n",
       "      <th>x29_Nov</th>\n",
       "      <th>x29_Oct</th>\n",
       "      <th>x29_sept.</th>\n",
       "      <th>x30_friday</th>\n",
       "      <th>x30_monday</th>\n",
       "      <th>x30_thurday</th>\n",
       "      <th>x30_tuesday</th>\n",
       "      <th>x30_wednesday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>102251</th>\n",
       "      <td>-0.247193</td>\n",
       "      <td>0.008193</td>\n",
       "      <td>-0.522953</td>\n",
       "      <td>-0.627133</td>\n",
       "      <td>1.719409</td>\n",
       "      <td>-0.110595</td>\n",
       "      <td>-0.522953</td>\n",
       "      <td>-1.210604</td>\n",
       "      <td>0.678020</td>\n",
       "      <td>-0.825505</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.088573</td>\n",
       "      <td>-0.398336</td>\n",
       "      <td>-0.045652</td>\n",
       "      <td>-0.122797</td>\n",
       "      <td>-0.269273</td>\n",
       "      <td>-0.059754</td>\n",
       "      <td>-0.054861</td>\n",
       "      <td>-0.474559</td>\n",
       "      <td>-0.460604</td>\n",
       "      <td>0.758801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144768</th>\n",
       "      <td>-0.386854</td>\n",
       "      <td>0.704865</td>\n",
       "      <td>1.087083</td>\n",
       "      <td>-1.650002</td>\n",
       "      <td>-1.852141</td>\n",
       "      <td>-0.492670</td>\n",
       "      <td>1.087083</td>\n",
       "      <td>-1.359340</td>\n",
       "      <td>-0.175212</td>\n",
       "      <td>-0.208328</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.088573</td>\n",
       "      <td>-0.398336</td>\n",
       "      <td>-0.045652</td>\n",
       "      <td>-0.122797</td>\n",
       "      <td>-0.269273</td>\n",
       "      <td>-0.059754</td>\n",
       "      <td>-0.054861</td>\n",
       "      <td>-0.474559</td>\n",
       "      <td>-0.460604</td>\n",
       "      <td>0.758801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28775</th>\n",
       "      <td>-0.147217</td>\n",
       "      <td>0.109888</td>\n",
       "      <td>-0.616624</td>\n",
       "      <td>0.149410</td>\n",
       "      <td>1.193672</td>\n",
       "      <td>0.999819</td>\n",
       "      <td>-0.616624</td>\n",
       "      <td>1.491931</td>\n",
       "      <td>-0.976708</td>\n",
       "      <td>-0.154350</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.088573</td>\n",
       "      <td>-0.398336</td>\n",
       "      <td>-0.045652</td>\n",
       "      <td>-0.122797</td>\n",
       "      <td>-0.269273</td>\n",
       "      <td>-0.059754</td>\n",
       "      <td>-0.054861</td>\n",
       "      <td>2.107220</td>\n",
       "      <td>-0.460604</td>\n",
       "      <td>-1.317868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38543</th>\n",
       "      <td>0.479052</td>\n",
       "      <td>-0.069270</td>\n",
       "      <td>0.848087</td>\n",
       "      <td>1.536878</td>\n",
       "      <td>-0.333788</td>\n",
       "      <td>-0.934403</td>\n",
       "      <td>0.848087</td>\n",
       "      <td>-0.048267</td>\n",
       "      <td>0.092830</td>\n",
       "      <td>0.345116</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.088573</td>\n",
       "      <td>-0.398336</td>\n",
       "      <td>-0.045652</td>\n",
       "      <td>-0.122797</td>\n",
       "      <td>-0.269273</td>\n",
       "      <td>16.735321</td>\n",
       "      <td>-0.054861</td>\n",
       "      <td>-0.474559</td>\n",
       "      <td>-0.460604</td>\n",
       "      <td>-1.317868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573</th>\n",
       "      <td>0.734758</td>\n",
       "      <td>0.456701</td>\n",
       "      <td>-1.294662</td>\n",
       "      <td>-0.116746</td>\n",
       "      <td>1.376315</td>\n",
       "      <td>-0.000061</td>\n",
       "      <td>-1.294662</td>\n",
       "      <td>-0.836994</td>\n",
       "      <td>-0.835163</td>\n",
       "      <td>1.752329</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.088573</td>\n",
       "      <td>-0.398336</td>\n",
       "      <td>-0.045652</td>\n",
       "      <td>-0.122797</td>\n",
       "      <td>-0.269273</td>\n",
       "      <td>-0.059754</td>\n",
       "      <td>-0.054861</td>\n",
       "      <td>-0.474559</td>\n",
       "      <td>-0.460604</td>\n",
       "      <td>0.758801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58335</th>\n",
       "      <td>0.435812</td>\n",
       "      <td>-2.023676</td>\n",
       "      <td>-1.013309</td>\n",
       "      <td>0.141425</td>\n",
       "      <td>-0.590504</td>\n",
       "      <td>-1.021231</td>\n",
       "      <td>-1.013309</td>\n",
       "      <td>0.497630</td>\n",
       "      <td>-0.110397</td>\n",
       "      <td>-0.640142</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.088573</td>\n",
       "      <td>-0.398336</td>\n",
       "      <td>-0.045652</td>\n",
       "      <td>-0.122797</td>\n",
       "      <td>-0.269273</td>\n",
       "      <td>-0.059754</td>\n",
       "      <td>-0.054861</td>\n",
       "      <td>-0.474559</td>\n",
       "      <td>-0.460604</td>\n",
       "      <td>0.758801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151696</th>\n",
       "      <td>-0.365096</td>\n",
       "      <td>0.470476</td>\n",
       "      <td>2.026942</td>\n",
       "      <td>-0.343560</td>\n",
       "      <td>-0.605596</td>\n",
       "      <td>1.428429</td>\n",
       "      <td>2.026942</td>\n",
       "      <td>-1.671419</td>\n",
       "      <td>1.555209</td>\n",
       "      <td>0.494920</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.088573</td>\n",
       "      <td>-0.398336</td>\n",
       "      <td>-0.045652</td>\n",
       "      <td>-0.122797</td>\n",
       "      <td>-0.269273</td>\n",
       "      <td>-0.059754</td>\n",
       "      <td>-0.054861</td>\n",
       "      <td>-0.474559</td>\n",
       "      <td>-0.460604</td>\n",
       "      <td>0.758801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119184</th>\n",
       "      <td>0.964732</td>\n",
       "      <td>1.490892</td>\n",
       "      <td>0.809631</td>\n",
       "      <td>0.416853</td>\n",
       "      <td>-0.381189</td>\n",
       "      <td>-0.002160</td>\n",
       "      <td>0.809631</td>\n",
       "      <td>0.296755</td>\n",
       "      <td>1.420391</td>\n",
       "      <td>0.456888</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.088573</td>\n",
       "      <td>-0.398336</td>\n",
       "      <td>-0.045652</td>\n",
       "      <td>-0.122797</td>\n",
       "      <td>-0.269273</td>\n",
       "      <td>-0.059754</td>\n",
       "      <td>-0.054861</td>\n",
       "      <td>-0.474559</td>\n",
       "      <td>-0.460604</td>\n",
       "      <td>0.758801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16311</th>\n",
       "      <td>0.738393</td>\n",
       "      <td>1.573768</td>\n",
       "      <td>-0.741380</td>\n",
       "      <td>0.701998</td>\n",
       "      <td>1.620661</td>\n",
       "      <td>0.478380</td>\n",
       "      <td>-0.741380</td>\n",
       "      <td>0.613702</td>\n",
       "      <td>-0.268291</td>\n",
       "      <td>0.324139</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.088573</td>\n",
       "      <td>-0.398336</td>\n",
       "      <td>-0.045652</td>\n",
       "      <td>-0.122797</td>\n",
       "      <td>-0.269273</td>\n",
       "      <td>-0.059754</td>\n",
       "      <td>-0.054861</td>\n",
       "      <td>-0.474559</td>\n",
       "      <td>2.171060</td>\n",
       "      <td>-1.317868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13177</th>\n",
       "      <td>-2.529110</td>\n",
       "      <td>-0.089395</td>\n",
       "      <td>-1.934012</td>\n",
       "      <td>-0.213446</td>\n",
       "      <td>-0.724026</td>\n",
       "      <td>1.245101</td>\n",
       "      <td>-1.934012</td>\n",
       "      <td>-1.117743</td>\n",
       "      <td>0.196991</td>\n",
       "      <td>0.804025</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.088573</td>\n",
       "      <td>-0.398336</td>\n",
       "      <td>-0.045652</td>\n",
       "      <td>-0.122797</td>\n",
       "      <td>-0.269273</td>\n",
       "      <td>-0.059754</td>\n",
       "      <td>-0.054861</td>\n",
       "      <td>-0.474559</td>\n",
       "      <td>-0.460604</td>\n",
       "      <td>0.758801</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>134633 rows × 67 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              x0        x1        x2        x3        x4        x5        x6  \\\n",
       "102251 -0.247193  0.008193 -0.522953 -0.627133  1.719409 -0.110595 -0.522953   \n",
       "144768 -0.386854  0.704865  1.087083 -1.650002 -1.852141 -0.492670  1.087083   \n",
       "28775  -0.147217  0.109888 -0.616624  0.149410  1.193672  0.999819 -0.616624   \n",
       "38543   0.479052 -0.069270  0.848087  1.536878 -0.333788 -0.934403  0.848087   \n",
       "573     0.734758  0.456701 -1.294662 -0.116746  1.376315 -0.000061 -1.294662   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "58335   0.435812 -2.023676 -1.013309  0.141425 -0.590504 -1.021231 -1.013309   \n",
       "151696 -0.365096  0.470476  2.026942 -0.343560 -0.605596  1.428429  2.026942   \n",
       "119184  0.964732  1.490892  0.809631  0.416853 -0.381189 -0.002160  0.809631   \n",
       "16311   0.738393  1.573768 -0.741380  0.701998  1.620661  0.478380 -0.741380   \n",
       "13177  -2.529110 -0.089395 -1.934012 -0.213446 -0.724026  1.245101 -1.934012   \n",
       "\n",
       "              x7        x8        x9  ...   x29_Mar   x29_May   x29_Nov  \\\n",
       "102251 -1.210604  0.678020 -0.825505  ... -0.088573 -0.398336 -0.045652   \n",
       "144768 -1.359340 -0.175212 -0.208328  ... -0.088573 -0.398336 -0.045652   \n",
       "28775   1.491931 -0.976708 -0.154350  ... -0.088573 -0.398336 -0.045652   \n",
       "38543  -0.048267  0.092830  0.345116  ... -0.088573 -0.398336 -0.045652   \n",
       "573    -0.836994 -0.835163  1.752329  ... -0.088573 -0.398336 -0.045652   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "58335   0.497630 -0.110397 -0.640142  ... -0.088573 -0.398336 -0.045652   \n",
       "151696 -1.671419  1.555209  0.494920  ... -0.088573 -0.398336 -0.045652   \n",
       "119184  0.296755  1.420391  0.456888  ... -0.088573 -0.398336 -0.045652   \n",
       "16311   0.613702 -0.268291  0.324139  ... -0.088573 -0.398336 -0.045652   \n",
       "13177  -1.117743  0.196991  0.804025  ... -0.088573 -0.398336 -0.045652   \n",
       "\n",
       "         x29_Oct  x29_sept.  x30_friday  x30_monday  x30_thurday  x30_tuesday  \\\n",
       "102251 -0.122797  -0.269273   -0.059754   -0.054861    -0.474559    -0.460604   \n",
       "144768 -0.122797  -0.269273   -0.059754   -0.054861    -0.474559    -0.460604   \n",
       "28775  -0.122797  -0.269273   -0.059754   -0.054861     2.107220    -0.460604   \n",
       "38543  -0.122797  -0.269273   16.735321   -0.054861    -0.474559    -0.460604   \n",
       "573    -0.122797  -0.269273   -0.059754   -0.054861    -0.474559    -0.460604   \n",
       "...          ...        ...         ...         ...          ...          ...   \n",
       "58335  -0.122797  -0.269273   -0.059754   -0.054861    -0.474559    -0.460604   \n",
       "151696 -0.122797  -0.269273   -0.059754   -0.054861    -0.474559    -0.460604   \n",
       "119184 -0.122797  -0.269273   -0.059754   -0.054861    -0.474559    -0.460604   \n",
       "16311  -0.122797  -0.269273   -0.059754   -0.054861    -0.474559     2.171060   \n",
       "13177  -0.122797  -0.269273   -0.059754   -0.054861    -0.474559    -0.460604   \n",
       "\n",
       "        x30_wednesday  \n",
       "102251       0.758801  \n",
       "144768       0.758801  \n",
       "28775       -1.317868  \n",
       "38543       -1.317868  \n",
       "573          0.758801  \n",
       "...               ...  \n",
       "58335        0.758801  \n",
       "151696       0.758801  \n",
       "119184       0.758801  \n",
       "16311       -1.317868  \n",
       "13177        0.758801  \n",
       "\n",
       "[134633 rows x 67 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sanity checks.\n",
    "\n",
    "train_scaled_features\n",
    "#The same as we had for X_train so that is expected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07d1673",
   "metadata": {},
   "source": [
    "## Fit and Predict.  \n",
    "- we will do cross validation on the entire dataset to evaluate models.  \n",
    "    - We will follow up with fit() on the train set and predict on the test set\n",
    "- We will use a custom function to evaluate Logistic Regression, Random Forest, XGBoost models on the dataset.  \n",
    "- We will use AUC Mean. \n",
    "    - The predicted y values will be estimated and tabulated as classification report.  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "342baf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Classification begins from here\n",
    "###Logistic Regression Classifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "def model_clf(train, test, y_train, y_test, clf):\n",
    "    \"\"\"\n",
    "    For this model training and prediction, we will do cross validation on the train to evaluate models.\n",
    "    We will follow up with fit() on the train set and predict on the test set\n",
    "    \"\"\"\n",
    "    if clf=='lr':\n",
    "        #Define the estimator object\n",
    "        #Here is logistic regression with l2 normalization. \n",
    "        #We do not do much tuning on this one yet.  \n",
    "        clf = LogisticRegression(penalty='l2', max_iter=1000, class_weight='balanced', C=1, random_state=42)\n",
    "        \n",
    "        print('working on Logistic regression')\n",
    "        #We cross validate for AUC score metric and return the mean scores.\n",
    "        #The higher the Area Under The Curve value the better the model.  \n",
    "        auc_scores = cross_val_score(clf, train, y_train,scoring='roc_auc', cv=10, n_jobs=-1)\n",
    "        auc_mean = np.mean(auc_scores)\n",
    "        auc_stdev = np.std(auc_scores)\n",
    "        clf_cl = clf.fit(train, y_train)\n",
    "        clf_pred = clf_cl.predict(test)\n",
    "        \n",
    "        #Assign column names to the feature importance 'scores'\n",
    "        feature_imp = list(zip(train.columns, clf_cl.coef_[0])) #we use this attribute to assess feature importance.\n",
    "        \n",
    "        #Here is the classification report that has accuracy, precision and recall.\n",
    "        class_rep = classification_report(y_test, clf_pred)\n",
    "        #return [class_rep, auc_mean, auc_stdev, feature_imp]\n",
    "        return [class_rep, clf_pred, auc_mean, auc_stdev, feature_imp]\n",
    "        \n",
    "###RandomForest Classifier############################################\n",
    "    elif clf=='rnd_clf':\n",
    "        #Define the estimator object\n",
    "        clf = RandomForestClassifier(class_weight='balanced', random_state=123)\n",
    "        print('Working on RandomForest') \n",
    "        \n",
    "         #We cross validate for AUC score metric and return the mean scores.\n",
    "        auc_scores = cross_val_score(clf, train, y_train,scoring='roc_auc', cv=10, n_jobs=-1)\n",
    "        auc_mean = np.mean(auc_scores)\n",
    "        auc_stdev = np.std(auc_scores)\n",
    "        clf_cl = clf.fit(train, y_train)\n",
    "        clf_pred = clf_cl.predict(test)\n",
    "        \n",
    "        #Here is the classification report that has accuracy, precision and recall.\n",
    "        class_rep = classification_report(y_test, clf_pred)\n",
    "        \n",
    "         #Assign column names to the feature importance 'scores'\n",
    "        feature_imp = list(zip(train.columns, clf_cl.feature_importances_)) #we use this attribute to assess feature importance.\n",
    "        return [class_rep, clf_pred, auc_mean, auc_stdev, feature_imp]\n",
    "        \n",
    "######XGBoost Classifier#################################################        \n",
    "    elif clf =='xgbr':\n",
    "#####Start with a randomsearch across parameter space##############\n",
    "        params = { 'max_depth': [3, 5, 6, 10, 15, 20],\n",
    "           'learning_rate': [0.01, 0.1, 0.2, 0.3],\n",
    "           'subsample': np.arange(0.5, 1.0, 0.1),\n",
    "           'colsample_bytree': np.arange(0.4, 1.0, 0.1),\n",
    "           'colsample_bylevel': np.arange(0.4, 1.0, 0.1),\n",
    "           'n_estimators': [100, 500, 1000]\n",
    "            \n",
    "                  \n",
    "                 }\n",
    "##We use the sci-kit learning api for the parameter search but final fit will be done with learning API\n",
    "        print('Starting XGB grid search')\n",
    "        xgb_clf = xgb.XGBClassifier(seed=123)\n",
    "        clf1 = RandomizedSearchCV(estimator=xgb_clf,\n",
    "                         param_distributions=params,\n",
    "                         scoring='roc_auc',\n",
    "                         n_iter=25,\n",
    "                         verbose=1)\n",
    "        search = clf1.fit(train, y_train)\n",
    "        \n",
    "#Selects the best parameters. \n",
    "#Note that XGBoost.train needs objective and number of target classes\n",
    "#We add those parameters to the dict output of best_params_\n",
    "        params = search.best_params_\n",
    "        params['objective']= 'binary:logistic'\n",
    "        #params['num_class']= 2\n",
    "        \n",
    "###Here we fit the bestparams to the learning API xgb.train()   \n",
    "        dtrain = xgb.DMatrix(train, y_train)\n",
    "        dtest=xgb.DMatrix(test, y_test)\n",
    "        evallist=[(dtest, 'eval'), (dtrain, 'train')]\n",
    "        num_round=500\n",
    "        \n",
    "        #We cross validate for AUC score metric and return the mean scores. 10 folds\n",
    "        my_model = xgb.train(params, dtrain, num_round, evallist, early_stopping_rounds=200)\n",
    "        feature_imp = my_model.get_score(importance_type='gain')\n",
    "        xgb_cv = xgb.cv(dtrain=dtrain, params=params,\n",
    "                            nfold=10, metrics={'auc'},\n",
    "                            seed=20, num_boost_round=1000,\n",
    "                        verbose_eval=True, early_stopping_rounds=500)\n",
    "        print('-'*20)\n",
    "        print('Working on XGboost............')\n",
    "        print('XGBoost cv result', xgb_cv)       \n",
    "        \n",
    "        \n",
    "        clf_pred_ = my_model.predict(dtest)       \n",
    "\n",
    "\n",
    "        # Area under curve scores across all the rounds of iteration\n",
    "        auc_mean = np.mean(xgb_cv['train-auc-mean'])\n",
    "        auc_stdev = np.std(xgb_cv['train-auc-mean'])   \n",
    "        \n",
    "        #XGBoost Learning API output prediction probability that needs further convertion\n",
    "        #prior to using classification_report\n",
    "        #we set the prediction probability threshold to 0.5\n",
    "        clf_pred = [] #List of the predicted class.\n",
    "        for i in clf_pred_:\n",
    "            if i > 0.5:\n",
    "                i = 1\n",
    "                clf_pred.append(i)\n",
    "            else:\n",
    "                i = 0\n",
    "                clf_pred.append(i)\n",
    "#The model errored out at this point and it takes 2 hours to complete the runs.\n",
    "#Even though I fixed the error, I want to be cautious.\n",
    "# I want other outputs in addition to \n",
    "        try:\n",
    "            class_rep = classification_report(clf_pred, y_test)\n",
    "        except Exception as e:\n",
    "            #If I get any error at all I still want the code to complete since I have other important\n",
    "            #variables already completed. \n",
    "            print(e)\n",
    "            class_rep = None  \n",
    "            \n",
    "            \n",
    "        return [class_rep, clf_pred, auc_mean, auc_stdev, feature_imp, y_test]\n",
    "\n",
    "    #return [class_rep, auc_mean, auc_stdev]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ec711550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on Logistic regression\n",
      "Working on RandomForest\n"
     ]
    }
   ],
   "source": [
    "classifiers = ['lr', 'rnd_clf', 'xgbr']\n",
    "classif_eval = {}\n",
    "\n",
    "for classifier in classifiers:\n",
    "    if classifier == 'lr':\n",
    "        classifier = 'logistic regression'\n",
    "        #classification_report, auc_mean, auc_mean = load_pred('lr', root)\n",
    "        classif_eval[classifier] = model_clf(train_scaled_features, test_scaled_features, y_train, y_test, 'lr')\n",
    "    if classifier == 'rnd_clf':\n",
    "        classifier = 'Random Forest'        \n",
    "        #classification_report, auc_mean, auc_mean = load_pred('rnd_clf', root)\n",
    "        classif_eval[classifier] = model_clf(train_scaled_features, test_scaled_features, y_train, y_test, 'rnd_clf')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa845ab5",
   "metadata": {},
   "source": [
    "**Train XGboost separately since it s process time takes longer than those of logistic and random forest.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0454813e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting XGB grid search\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:26:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:26:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:27:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:27:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:27:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:28:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:35:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:42:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:49:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:55:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:00:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:06:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:13:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:19:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:26:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:32:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:33:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:34:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:35:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:35:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:36:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:37:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:37:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:38:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:38:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:39:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:45:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:52:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:01:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:09:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:15:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:17:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:19:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:21:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:23:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:25:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:26:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:27:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:28:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:29:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:30:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:30:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:31:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:31:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:32:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:32:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:33:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:34:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:35:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:36:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:36:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:48:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:59:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:13:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:26:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:37:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:40:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:43:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:47:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:50:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:53:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:59:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:05:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:12:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:18:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:24:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:29:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:34:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:39:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:44:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:49:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:49:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:50:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:50:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:50:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:51:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:59:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:07:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:14:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:22:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:30:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:32:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:34:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:37:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:39:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:42:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:47:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:52:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:57:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:03:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:08:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:10:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:12:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:14:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:16:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:18:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:20:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:22:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:23:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:25:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:27:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:35:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:42:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:49:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:57:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:04:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:07:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:09:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:12:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:15:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:18:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:26:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:35:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:45:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:54:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:04:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:11:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:18:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:25:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:33:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:39:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:51:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:04:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:16:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:28:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olani\\anaconda3\\envs\\Machine Learning 1\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:40:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[00:54:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"n_estimators\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[00:54:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\teval-logloss:0.63961\ttrain-logloss:0.62551\n",
      "[1]\teval-logloss:0.58979\ttrain-logloss:0.56603\n",
      "[2]\teval-logloss:0.54747\ttrain-logloss:0.51433\n",
      "[3]\teval-logloss:0.51841\ttrain-logloss:0.47407\n",
      "[4]\teval-logloss:0.49198\ttrain-logloss:0.43907\n",
      "[5]\teval-logloss:0.46310\ttrain-logloss:0.40340\n",
      "[6]\teval-logloss:0.43659\ttrain-logloss:0.37172\n",
      "[7]\teval-logloss:0.41723\ttrain-logloss:0.34565\n",
      "[8]\teval-logloss:0.39804\ttrain-logloss:0.32080\n",
      "[9]\teval-logloss:0.37972\ttrain-logloss:0.29814\n",
      "[10]\teval-logloss:0.36750\ttrain-logloss:0.28034\n",
      "[11]\teval-logloss:0.35458\ttrain-logloss:0.26325\n",
      "[12]\teval-logloss:0.33805\ttrain-logloss:0.24433\n",
      "[13]\teval-logloss:0.32623\ttrain-logloss:0.22905\n",
      "[14]\teval-logloss:0.31434\ttrain-logloss:0.21464\n",
      "[15]\teval-logloss:0.30256\ttrain-logloss:0.20080\n",
      "[16]\teval-logloss:0.29319\ttrain-logloss:0.18914\n",
      "[17]\teval-logloss:0.28450\ttrain-logloss:0.17823\n",
      "[18]\teval-logloss:0.27572\ttrain-logloss:0.16772\n",
      "[19]\teval-logloss:0.26910\ttrain-logloss:0.15918\n",
      "[20]\teval-logloss:0.26188\ttrain-logloss:0.15039\n",
      "[21]\teval-logloss:0.25569\ttrain-logloss:0.14247\n",
      "[22]\teval-logloss:0.25082\ttrain-logloss:0.13580\n",
      "[23]\teval-logloss:0.24657\ttrain-logloss:0.12973\n",
      "[24]\teval-logloss:0.24276\ttrain-logloss:0.12398\n",
      "[25]\teval-logloss:0.23721\ttrain-logloss:0.11760\n",
      "[26]\teval-logloss:0.23300\ttrain-logloss:0.11253\n",
      "[27]\teval-logloss:0.22839\ttrain-logloss:0.10720\n",
      "[28]\teval-logloss:0.22454\ttrain-logloss:0.10246\n",
      "[29]\teval-logloss:0.22020\ttrain-logloss:0.09764\n",
      "[30]\teval-logloss:0.21594\ttrain-logloss:0.09291\n",
      "[31]\teval-logloss:0.21272\ttrain-logloss:0.08902\n",
      "[32]\teval-logloss:0.21027\ttrain-logloss:0.08570\n",
      "[33]\teval-logloss:0.20809\ttrain-logloss:0.08237\n",
      "[34]\teval-logloss:0.20531\ttrain-logloss:0.07910\n",
      "[35]\teval-logloss:0.20226\ttrain-logloss:0.07594\n",
      "[36]\teval-logloss:0.20060\ttrain-logloss:0.07355\n",
      "[37]\teval-logloss:0.19838\ttrain-logloss:0.07091\n",
      "[38]\teval-logloss:0.19640\ttrain-logloss:0.06839\n",
      "[39]\teval-logloss:0.19400\ttrain-logloss:0.06573\n",
      "[40]\teval-logloss:0.19223\ttrain-logloss:0.06349\n",
      "[41]\teval-logloss:0.19054\ttrain-logloss:0.06148\n",
      "[42]\teval-logloss:0.18861\ttrain-logloss:0.05924\n",
      "[43]\teval-logloss:0.18751\ttrain-logloss:0.05758\n",
      "[44]\teval-logloss:0.18591\ttrain-logloss:0.05556\n",
      "[45]\teval-logloss:0.18421\ttrain-logloss:0.05361\n",
      "[46]\teval-logloss:0.18319\ttrain-logloss:0.05209\n",
      "[47]\teval-logloss:0.18233\ttrain-logloss:0.05062\n",
      "[48]\teval-logloss:0.18134\ttrain-logloss:0.04912\n",
      "[49]\teval-logloss:0.18035\ttrain-logloss:0.04774\n",
      "[50]\teval-logloss:0.17955\ttrain-logloss:0.04641\n",
      "[51]\teval-logloss:0.17891\ttrain-logloss:0.04527\n",
      "[52]\teval-logloss:0.17792\ttrain-logloss:0.04403\n",
      "[53]\teval-logloss:0.17638\ttrain-logloss:0.04262\n",
      "[54]\teval-logloss:0.17536\ttrain-logloss:0.04140\n",
      "[55]\teval-logloss:0.17492\ttrain-logloss:0.04060\n",
      "[56]\teval-logloss:0.17414\ttrain-logloss:0.03964\n",
      "[57]\teval-logloss:0.17337\ttrain-logloss:0.03870\n",
      "[58]\teval-logloss:0.17289\ttrain-logloss:0.03782\n",
      "[59]\teval-logloss:0.17255\ttrain-logloss:0.03696\n",
      "[60]\teval-logloss:0.17240\ttrain-logloss:0.03629\n",
      "[61]\teval-logloss:0.17151\ttrain-logloss:0.03533\n",
      "[62]\teval-logloss:0.17080\ttrain-logloss:0.03453\n",
      "[63]\teval-logloss:0.17007\ttrain-logloss:0.03363\n",
      "[64]\teval-logloss:0.16960\ttrain-logloss:0.03279\n",
      "[65]\teval-logloss:0.16899\ttrain-logloss:0.03206\n",
      "[66]\teval-logloss:0.16834\ttrain-logloss:0.03135\n",
      "[67]\teval-logloss:0.16784\ttrain-logloss:0.03069\n",
      "[68]\teval-logloss:0.16744\ttrain-logloss:0.03003\n",
      "[69]\teval-logloss:0.16698\ttrain-logloss:0.02935\n",
      "[70]\teval-logloss:0.16669\ttrain-logloss:0.02877\n",
      "[71]\teval-logloss:0.16632\ttrain-logloss:0.02818\n",
      "[72]\teval-logloss:0.16628\ttrain-logloss:0.02780\n",
      "[73]\teval-logloss:0.16537\ttrain-logloss:0.02707\n",
      "[74]\teval-logloss:0.16499\ttrain-logloss:0.02663\n",
      "[75]\teval-logloss:0.16468\ttrain-logloss:0.02623\n",
      "[76]\teval-logloss:0.16432\ttrain-logloss:0.02574\n",
      "[77]\teval-logloss:0.16363\ttrain-logloss:0.02517\n",
      "[78]\teval-logloss:0.16328\ttrain-logloss:0.02475\n",
      "[79]\teval-logloss:0.16287\ttrain-logloss:0.02422\n",
      "[80]\teval-logloss:0.16273\ttrain-logloss:0.02391\n",
      "[81]\teval-logloss:0.16244\ttrain-logloss:0.02347\n",
      "[82]\teval-logloss:0.16240\ttrain-logloss:0.02319\n",
      "[83]\teval-logloss:0.16250\ttrain-logloss:0.02292\n",
      "[84]\teval-logloss:0.16233\ttrain-logloss:0.02250\n",
      "[85]\teval-logloss:0.16184\ttrain-logloss:0.02204\n",
      "[86]\teval-logloss:0.16137\ttrain-logloss:0.02168\n",
      "[87]\teval-logloss:0.16112\ttrain-logloss:0.02129\n",
      "[88]\teval-logloss:0.16062\ttrain-logloss:0.02083\n",
      "[89]\teval-logloss:0.16013\ttrain-logloss:0.02043\n",
      "[90]\teval-logloss:0.15992\ttrain-logloss:0.02010\n",
      "[91]\teval-logloss:0.15979\ttrain-logloss:0.01978\n",
      "[92]\teval-logloss:0.15952\ttrain-logloss:0.01944\n",
      "[93]\teval-logloss:0.15951\ttrain-logloss:0.01923\n",
      "[94]\teval-logloss:0.15942\ttrain-logloss:0.01897\n",
      "[95]\teval-logloss:0.15913\ttrain-logloss:0.01864\n",
      "[96]\teval-logloss:0.15895\ttrain-logloss:0.01842\n",
      "[97]\teval-logloss:0.15884\ttrain-logloss:0.01815\n",
      "[98]\teval-logloss:0.15900\ttrain-logloss:0.01794\n",
      "[99]\teval-logloss:0.15884\ttrain-logloss:0.01767\n",
      "[100]\teval-logloss:0.15853\ttrain-logloss:0.01740\n",
      "[101]\teval-logloss:0.15852\ttrain-logloss:0.01727\n",
      "[102]\teval-logloss:0.15835\ttrain-logloss:0.01702\n",
      "[103]\teval-logloss:0.15822\ttrain-logloss:0.01689\n",
      "[104]\teval-logloss:0.15816\ttrain-logloss:0.01670\n",
      "[105]\teval-logloss:0.15785\ttrain-logloss:0.01643\n",
      "[106]\teval-logloss:0.15764\ttrain-logloss:0.01619\n",
      "[107]\teval-logloss:0.15759\ttrain-logloss:0.01597\n",
      "[108]\teval-logloss:0.15752\ttrain-logloss:0.01574\n",
      "[109]\teval-logloss:0.15755\ttrain-logloss:0.01556\n",
      "[110]\teval-logloss:0.15739\ttrain-logloss:0.01534\n",
      "[111]\teval-logloss:0.15743\ttrain-logloss:0.01524\n",
      "[112]\teval-logloss:0.15741\ttrain-logloss:0.01510\n",
      "[113]\teval-logloss:0.15732\ttrain-logloss:0.01490\n",
      "[114]\teval-logloss:0.15731\ttrain-logloss:0.01471\n",
      "[115]\teval-logloss:0.15730\ttrain-logloss:0.01462\n",
      "[116]\teval-logloss:0.15728\ttrain-logloss:0.01443\n",
      "[117]\teval-logloss:0.15716\ttrain-logloss:0.01427\n",
      "[118]\teval-logloss:0.15708\ttrain-logloss:0.01416\n",
      "[119]\teval-logloss:0.15702\ttrain-logloss:0.01403\n",
      "[120]\teval-logloss:0.15702\ttrain-logloss:0.01390\n",
      "[121]\teval-logloss:0.15694\ttrain-logloss:0.01370\n",
      "[122]\teval-logloss:0.15694\ttrain-logloss:0.01357\n",
      "[123]\teval-logloss:0.15689\ttrain-logloss:0.01347\n",
      "[124]\teval-logloss:0.15680\ttrain-logloss:0.01330\n",
      "[125]\teval-logloss:0.15685\ttrain-logloss:0.01317\n",
      "[126]\teval-logloss:0.15676\ttrain-logloss:0.01303\n",
      "[127]\teval-logloss:0.15673\ttrain-logloss:0.01289\n",
      "[128]\teval-logloss:0.15661\ttrain-logloss:0.01272\n",
      "[129]\teval-logloss:0.15661\ttrain-logloss:0.01263\n",
      "[130]\teval-logloss:0.15654\ttrain-logloss:0.01248\n",
      "[131]\teval-logloss:0.15647\ttrain-logloss:0.01237\n",
      "[132]\teval-logloss:0.15643\ttrain-logloss:0.01223\n",
      "[133]\teval-logloss:0.15642\ttrain-logloss:0.01208\n",
      "[134]\teval-logloss:0.15643\ttrain-logloss:0.01201\n",
      "[135]\teval-logloss:0.15648\ttrain-logloss:0.01191\n",
      "[136]\teval-logloss:0.15648\ttrain-logloss:0.01181\n",
      "[137]\teval-logloss:0.15635\ttrain-logloss:0.01169\n",
      "[138]\teval-logloss:0.15621\ttrain-logloss:0.01155\n",
      "[139]\teval-logloss:0.15626\ttrain-logloss:0.01142\n",
      "[140]\teval-logloss:0.15620\ttrain-logloss:0.01131\n",
      "[141]\teval-logloss:0.15620\ttrain-logloss:0.01120\n",
      "[142]\teval-logloss:0.15613\ttrain-logloss:0.01107\n",
      "[143]\teval-logloss:0.15618\ttrain-logloss:0.01096\n",
      "[144]\teval-logloss:0.15607\ttrain-logloss:0.01083\n",
      "[145]\teval-logloss:0.15607\ttrain-logloss:0.01074\n",
      "[146]\teval-logloss:0.15596\ttrain-logloss:0.01064\n",
      "[147]\teval-logloss:0.15607\ttrain-logloss:0.01058\n",
      "[148]\teval-logloss:0.15591\ttrain-logloss:0.01047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[149]\teval-logloss:0.15588\ttrain-logloss:0.01037\n",
      "[150]\teval-logloss:0.15590\ttrain-logloss:0.01027\n",
      "[151]\teval-logloss:0.15582\ttrain-logloss:0.01019\n",
      "[152]\teval-logloss:0.15578\ttrain-logloss:0.01013\n",
      "[153]\teval-logloss:0.15575\ttrain-logloss:0.01005\n",
      "[154]\teval-logloss:0.15573\ttrain-logloss:0.00996\n",
      "[155]\teval-logloss:0.15574\ttrain-logloss:0.00988\n",
      "[156]\teval-logloss:0.15571\ttrain-logloss:0.00979\n",
      "[157]\teval-logloss:0.15582\ttrain-logloss:0.00970\n",
      "[158]\teval-logloss:0.15583\ttrain-logloss:0.00961\n",
      "[159]\teval-logloss:0.15580\ttrain-logloss:0.00953\n",
      "[160]\teval-logloss:0.15583\ttrain-logloss:0.00946\n",
      "[161]\teval-logloss:0.15571\ttrain-logloss:0.00936\n",
      "[162]\teval-logloss:0.15566\ttrain-logloss:0.00927\n",
      "[163]\teval-logloss:0.15554\ttrain-logloss:0.00918\n",
      "[164]\teval-logloss:0.15556\ttrain-logloss:0.00913\n",
      "[165]\teval-logloss:0.15553\ttrain-logloss:0.00904\n",
      "[166]\teval-logloss:0.15558\ttrain-logloss:0.00896\n",
      "[167]\teval-logloss:0.15556\ttrain-logloss:0.00888\n",
      "[168]\teval-logloss:0.15558\ttrain-logloss:0.00881\n",
      "[169]\teval-logloss:0.15560\ttrain-logloss:0.00876\n",
      "[170]\teval-logloss:0.15564\ttrain-logloss:0.00871\n",
      "[171]\teval-logloss:0.15556\ttrain-logloss:0.00866\n",
      "[172]\teval-logloss:0.15540\ttrain-logloss:0.00857\n",
      "[173]\teval-logloss:0.15532\ttrain-logloss:0.00849\n",
      "[174]\teval-logloss:0.15529\ttrain-logloss:0.00841\n",
      "[175]\teval-logloss:0.15524\ttrain-logloss:0.00834\n",
      "[176]\teval-logloss:0.15523\ttrain-logloss:0.00826\n",
      "[177]\teval-logloss:0.15526\ttrain-logloss:0.00818\n",
      "[178]\teval-logloss:0.15534\ttrain-logloss:0.00812\n",
      "[179]\teval-logloss:0.15534\ttrain-logloss:0.00809\n",
      "[180]\teval-logloss:0.15537\ttrain-logloss:0.00806\n",
      "[181]\teval-logloss:0.15540\ttrain-logloss:0.00802\n",
      "[182]\teval-logloss:0.15547\ttrain-logloss:0.00796\n",
      "[183]\teval-logloss:0.15554\ttrain-logloss:0.00791\n",
      "[184]\teval-logloss:0.15552\ttrain-logloss:0.00785\n",
      "[185]\teval-logloss:0.15553\ttrain-logloss:0.00781\n",
      "[186]\teval-logloss:0.15558\ttrain-logloss:0.00776\n",
      "[187]\teval-logloss:0.15558\ttrain-logloss:0.00770\n",
      "[188]\teval-logloss:0.15561\ttrain-logloss:0.00764\n",
      "[189]\teval-logloss:0.15570\ttrain-logloss:0.00760\n",
      "[190]\teval-logloss:0.15576\ttrain-logloss:0.00754\n",
      "[191]\teval-logloss:0.15581\ttrain-logloss:0.00750\n",
      "[192]\teval-logloss:0.15588\ttrain-logloss:0.00747\n",
      "[193]\teval-logloss:0.15593\ttrain-logloss:0.00742\n",
      "[194]\teval-logloss:0.15598\ttrain-logloss:0.00739\n",
      "[195]\teval-logloss:0.15594\ttrain-logloss:0.00734\n",
      "[196]\teval-logloss:0.15591\ttrain-logloss:0.00728\n",
      "[197]\teval-logloss:0.15598\ttrain-logloss:0.00724\n",
      "[198]\teval-logloss:0.15594\ttrain-logloss:0.00718\n",
      "[199]\teval-logloss:0.15590\ttrain-logloss:0.00715\n",
      "[200]\teval-logloss:0.15583\ttrain-logloss:0.00709\n",
      "[201]\teval-logloss:0.15583\ttrain-logloss:0.00704\n",
      "[202]\teval-logloss:0.15580\ttrain-logloss:0.00700\n",
      "[203]\teval-logloss:0.15581\ttrain-logloss:0.00695\n",
      "[204]\teval-logloss:0.15578\ttrain-logloss:0.00692\n",
      "[205]\teval-logloss:0.15577\ttrain-logloss:0.00688\n",
      "[206]\teval-logloss:0.15565\ttrain-logloss:0.00682\n",
      "[207]\teval-logloss:0.15565\ttrain-logloss:0.00677\n",
      "[208]\teval-logloss:0.15567\ttrain-logloss:0.00674\n",
      "[209]\teval-logloss:0.15577\ttrain-logloss:0.00669\n",
      "[210]\teval-logloss:0.15582\ttrain-logloss:0.00666\n",
      "[211]\teval-logloss:0.15585\ttrain-logloss:0.00662\n",
      "[212]\teval-logloss:0.15584\ttrain-logloss:0.00657\n",
      "[213]\teval-logloss:0.15588\ttrain-logloss:0.00654\n",
      "[214]\teval-logloss:0.15576\ttrain-logloss:0.00648\n",
      "[215]\teval-logloss:0.15580\ttrain-logloss:0.00643\n",
      "[216]\teval-logloss:0.15588\ttrain-logloss:0.00640\n",
      "[217]\teval-logloss:0.15594\ttrain-logloss:0.00636\n",
      "[218]\teval-logloss:0.15597\ttrain-logloss:0.00632\n",
      "[219]\teval-logloss:0.15605\ttrain-logloss:0.00628\n",
      "[220]\teval-logloss:0.15595\ttrain-logloss:0.00623\n",
      "[221]\teval-logloss:0.15590\ttrain-logloss:0.00619\n",
      "[222]\teval-logloss:0.15596\ttrain-logloss:0.00616\n",
      "[223]\teval-logloss:0.15594\ttrain-logloss:0.00612\n",
      "[224]\teval-logloss:0.15599\ttrain-logloss:0.00610\n",
      "[225]\teval-logloss:0.15594\ttrain-logloss:0.00605\n",
      "[226]\teval-logloss:0.15595\ttrain-logloss:0.00602\n",
      "[227]\teval-logloss:0.15602\ttrain-logloss:0.00598\n",
      "[228]\teval-logloss:0.15606\ttrain-logloss:0.00596\n",
      "[229]\teval-logloss:0.15604\ttrain-logloss:0.00592\n",
      "[230]\teval-logloss:0.15605\ttrain-logloss:0.00589\n",
      "[231]\teval-logloss:0.15603\ttrain-logloss:0.00586\n",
      "[232]\teval-logloss:0.15604\ttrain-logloss:0.00583\n",
      "[233]\teval-logloss:0.15607\ttrain-logloss:0.00579\n",
      "[234]\teval-logloss:0.15615\ttrain-logloss:0.00575\n",
      "[235]\teval-logloss:0.15615\ttrain-logloss:0.00573\n",
      "[236]\teval-logloss:0.15622\ttrain-logloss:0.00570\n",
      "[237]\teval-logloss:0.15626\ttrain-logloss:0.00568\n",
      "[238]\teval-logloss:0.15625\ttrain-logloss:0.00565\n",
      "[239]\teval-logloss:0.15631\ttrain-logloss:0.00562\n",
      "[240]\teval-logloss:0.15638\ttrain-logloss:0.00559\n",
      "[241]\teval-logloss:0.15637\ttrain-logloss:0.00556\n",
      "[242]\teval-logloss:0.15639\ttrain-logloss:0.00554\n",
      "[243]\teval-logloss:0.15647\ttrain-logloss:0.00551\n",
      "[244]\teval-logloss:0.15654\ttrain-logloss:0.00548\n",
      "[245]\teval-logloss:0.15652\ttrain-logloss:0.00544\n",
      "[246]\teval-logloss:0.15654\ttrain-logloss:0.00542\n",
      "[247]\teval-logloss:0.15660\ttrain-logloss:0.00539\n",
      "[248]\teval-logloss:0.15660\ttrain-logloss:0.00535\n",
      "[249]\teval-logloss:0.15656\ttrain-logloss:0.00532\n",
      "[250]\teval-logloss:0.15660\ttrain-logloss:0.00530\n",
      "[251]\teval-logloss:0.15664\ttrain-logloss:0.00528\n",
      "[252]\teval-logloss:0.15653\ttrain-logloss:0.00524\n",
      "[253]\teval-logloss:0.15657\ttrain-logloss:0.00521\n",
      "[254]\teval-logloss:0.15662\ttrain-logloss:0.00519\n",
      "[255]\teval-logloss:0.15665\ttrain-logloss:0.00517\n",
      "[256]\teval-logloss:0.15671\ttrain-logloss:0.00514\n",
      "[257]\teval-logloss:0.15674\ttrain-logloss:0.00512\n",
      "[258]\teval-logloss:0.15678\ttrain-logloss:0.00510\n",
      "[259]\teval-logloss:0.15678\ttrain-logloss:0.00508\n",
      "[260]\teval-logloss:0.15680\ttrain-logloss:0.00505\n",
      "[261]\teval-logloss:0.15689\ttrain-logloss:0.00502\n",
      "[262]\teval-logloss:0.15688\ttrain-logloss:0.00500\n",
      "[263]\teval-logloss:0.15688\ttrain-logloss:0.00496\n",
      "[264]\teval-logloss:0.15690\ttrain-logloss:0.00494\n",
      "[265]\teval-logloss:0.15684\ttrain-logloss:0.00491\n",
      "[266]\teval-logloss:0.15689\ttrain-logloss:0.00489\n",
      "[267]\teval-logloss:0.15689\ttrain-logloss:0.00487\n",
      "[268]\teval-logloss:0.15689\ttrain-logloss:0.00485\n",
      "[269]\teval-logloss:0.15690\ttrain-logloss:0.00482\n",
      "[270]\teval-logloss:0.15689\ttrain-logloss:0.00481\n",
      "[271]\teval-logloss:0.15695\ttrain-logloss:0.00478\n",
      "[272]\teval-logloss:0.15701\ttrain-logloss:0.00476\n",
      "[273]\teval-logloss:0.15704\ttrain-logloss:0.00473\n",
      "[274]\teval-logloss:0.15708\ttrain-logloss:0.00472\n",
      "[275]\teval-logloss:0.15713\ttrain-logloss:0.00470\n",
      "[276]\teval-logloss:0.15720\ttrain-logloss:0.00468\n",
      "[277]\teval-logloss:0.15718\ttrain-logloss:0.00466\n",
      "[278]\teval-logloss:0.15719\ttrain-logloss:0.00463\n",
      "[279]\teval-logloss:0.15718\ttrain-logloss:0.00460\n",
      "[280]\teval-logloss:0.15718\ttrain-logloss:0.00459\n",
      "[281]\teval-logloss:0.15720\ttrain-logloss:0.00456\n",
      "[282]\teval-logloss:0.15724\ttrain-logloss:0.00454\n",
      "[283]\teval-logloss:0.15722\ttrain-logloss:0.00452\n",
      "[284]\teval-logloss:0.15734\ttrain-logloss:0.00450\n",
      "[285]\teval-logloss:0.15735\ttrain-logloss:0.00448\n",
      "[286]\teval-logloss:0.15741\ttrain-logloss:0.00446\n",
      "[287]\teval-logloss:0.15742\ttrain-logloss:0.00444\n",
      "[288]\teval-logloss:0.15747\ttrain-logloss:0.00442\n",
      "[289]\teval-logloss:0.15751\ttrain-logloss:0.00439\n",
      "[290]\teval-logloss:0.15757\ttrain-logloss:0.00438\n",
      "[291]\teval-logloss:0.15759\ttrain-logloss:0.00436\n",
      "[292]\teval-logloss:0.15751\ttrain-logloss:0.00433\n",
      "[293]\teval-logloss:0.15753\ttrain-logloss:0.00432\n",
      "[294]\teval-logloss:0.15755\ttrain-logloss:0.00430\n",
      "[295]\teval-logloss:0.15757\ttrain-logloss:0.00428\n",
      "[296]\teval-logloss:0.15758\ttrain-logloss:0.00426\n",
      "[297]\teval-logloss:0.15758\ttrain-logloss:0.00424\n",
      "[298]\teval-logloss:0.15753\ttrain-logloss:0.00422\n",
      "[299]\teval-logloss:0.15757\ttrain-logloss:0.00420\n",
      "[300]\teval-logloss:0.15758\ttrain-logloss:0.00418\n",
      "[301]\teval-logloss:0.15759\ttrain-logloss:0.00417\n",
      "[302]\teval-logloss:0.15758\ttrain-logloss:0.00415\n",
      "[303]\teval-logloss:0.15761\ttrain-logloss:0.00413\n",
      "[304]\teval-logloss:0.15759\ttrain-logloss:0.00411\n",
      "[305]\teval-logloss:0.15759\ttrain-logloss:0.00409\n",
      "[306]\teval-logloss:0.15764\ttrain-logloss:0.00407\n",
      "[307]\teval-logloss:0.15765\ttrain-logloss:0.00406\n",
      "[308]\teval-logloss:0.15770\ttrain-logloss:0.00405\n",
      "[309]\teval-logloss:0.15770\ttrain-logloss:0.00404\n",
      "[310]\teval-logloss:0.15772\ttrain-logloss:0.00403\n",
      "[311]\teval-logloss:0.15773\ttrain-logloss:0.00401\n",
      "[312]\teval-logloss:0.15773\ttrain-logloss:0.00400\n",
      "[313]\teval-logloss:0.15774\ttrain-logloss:0.00398\n",
      "[314]\teval-logloss:0.15777\ttrain-logloss:0.00396\n",
      "[315]\teval-logloss:0.15782\ttrain-logloss:0.00395\n",
      "[316]\teval-logloss:0.15783\ttrain-logloss:0.00393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[317]\teval-logloss:0.15789\ttrain-logloss:0.00392\n",
      "[318]\teval-logloss:0.15799\ttrain-logloss:0.00391\n",
      "[319]\teval-logloss:0.15800\ttrain-logloss:0.00389\n",
      "[320]\teval-logloss:0.15804\ttrain-logloss:0.00387\n",
      "[321]\teval-logloss:0.15811\ttrain-logloss:0.00386\n",
      "[322]\teval-logloss:0.15820\ttrain-logloss:0.00385\n",
      "[323]\teval-logloss:0.15818\ttrain-logloss:0.00383\n",
      "[324]\teval-logloss:0.15822\ttrain-logloss:0.00382\n",
      "[325]\teval-logloss:0.15820\ttrain-logloss:0.00381\n",
      "[326]\teval-logloss:0.15821\ttrain-logloss:0.00379\n",
      "[327]\teval-logloss:0.15823\ttrain-logloss:0.00378\n",
      "[328]\teval-logloss:0.15826\ttrain-logloss:0.00377\n",
      "[329]\teval-logloss:0.15831\ttrain-logloss:0.00376\n",
      "[330]\teval-logloss:0.15835\ttrain-logloss:0.00374\n",
      "[331]\teval-logloss:0.15839\ttrain-logloss:0.00373\n",
      "[332]\teval-logloss:0.15843\ttrain-logloss:0.00371\n",
      "[333]\teval-logloss:0.15840\ttrain-logloss:0.00370\n",
      "[334]\teval-logloss:0.15845\ttrain-logloss:0.00368\n",
      "[335]\teval-logloss:0.15846\ttrain-logloss:0.00368\n",
      "[336]\teval-logloss:0.15850\ttrain-logloss:0.00366\n",
      "[337]\teval-logloss:0.15851\ttrain-logloss:0.00365\n",
      "[338]\teval-logloss:0.15863\ttrain-logloss:0.00363\n",
      "[339]\teval-logloss:0.15864\ttrain-logloss:0.00362\n",
      "[340]\teval-logloss:0.15866\ttrain-logloss:0.00361\n",
      "[341]\teval-logloss:0.15871\ttrain-logloss:0.00360\n",
      "[342]\teval-logloss:0.15876\ttrain-logloss:0.00358\n",
      "[343]\teval-logloss:0.15883\ttrain-logloss:0.00357\n",
      "[344]\teval-logloss:0.15885\ttrain-logloss:0.00356\n",
      "[345]\teval-logloss:0.15886\ttrain-logloss:0.00354\n",
      "[346]\teval-logloss:0.15890\ttrain-logloss:0.00353\n",
      "[347]\teval-logloss:0.15891\ttrain-logloss:0.00352\n",
      "[348]\teval-logloss:0.15893\ttrain-logloss:0.00351\n",
      "[349]\teval-logloss:0.15894\ttrain-logloss:0.00350\n",
      "[350]\teval-logloss:0.15892\ttrain-logloss:0.00348\n",
      "[351]\teval-logloss:0.15895\ttrain-logloss:0.00347\n",
      "[352]\teval-logloss:0.15897\ttrain-logloss:0.00346\n",
      "[353]\teval-logloss:0.15901\ttrain-logloss:0.00345\n",
      "[354]\teval-logloss:0.15905\ttrain-logloss:0.00344\n",
      "[355]\teval-logloss:0.15911\ttrain-logloss:0.00342\n",
      "[356]\teval-logloss:0.15913\ttrain-logloss:0.00341\n",
      "[357]\teval-logloss:0.15913\ttrain-logloss:0.00339\n",
      "[358]\teval-logloss:0.15912\ttrain-logloss:0.00338\n",
      "[359]\teval-logloss:0.15912\ttrain-logloss:0.00337\n",
      "[360]\teval-logloss:0.15913\ttrain-logloss:0.00336\n",
      "[361]\teval-logloss:0.15917\ttrain-logloss:0.00335\n",
      "[362]\teval-logloss:0.15920\ttrain-logloss:0.00334\n",
      "[363]\teval-logloss:0.15918\ttrain-logloss:0.00333\n",
      "[364]\teval-logloss:0.15923\ttrain-logloss:0.00332\n",
      "[365]\teval-logloss:0.15920\ttrain-logloss:0.00330\n",
      "[366]\teval-logloss:0.15921\ttrain-logloss:0.00329\n",
      "[367]\teval-logloss:0.15926\ttrain-logloss:0.00328\n",
      "[368]\teval-logloss:0.15932\ttrain-logloss:0.00327\n",
      "[369]\teval-logloss:0.15929\ttrain-logloss:0.00326\n",
      "[370]\teval-logloss:0.15931\ttrain-logloss:0.00325\n",
      "[371]\teval-logloss:0.15931\ttrain-logloss:0.00323\n",
      "[372]\teval-logloss:0.15932\ttrain-logloss:0.00322\n",
      "[373]\teval-logloss:0.15942\ttrain-logloss:0.00320\n",
      "[374]\teval-logloss:0.15945\ttrain-logloss:0.00320\n",
      "[375]\teval-logloss:0.15951\ttrain-logloss:0.00318\n",
      "[376]\teval-logloss:0.15957\ttrain-logloss:0.00317\n",
      "[377]\teval-logloss:0.15958\ttrain-logloss:0.00317\n",
      "[378]\teval-logloss:0.15958\ttrain-logloss:0.00316\n",
      "[379]\teval-logloss:0.15961\ttrain-logloss:0.00315\n",
      "[380]\teval-logloss:0.15961\ttrain-logloss:0.00314\n",
      "[381]\teval-logloss:0.15962\ttrain-logloss:0.00314\n",
      "[382]\teval-logloss:0.15971\ttrain-logloss:0.00313\n",
      "[383]\teval-logloss:0.15979\ttrain-logloss:0.00312\n",
      "[384]\teval-logloss:0.15981\ttrain-logloss:0.00311\n",
      "[385]\teval-logloss:0.15983\ttrain-logloss:0.00310\n",
      "[386]\teval-logloss:0.15988\ttrain-logloss:0.00309\n",
      "[387]\teval-logloss:0.15986\ttrain-logloss:0.00308\n",
      "[388]\teval-logloss:0.15996\ttrain-logloss:0.00307\n",
      "[389]\teval-logloss:0.15999\ttrain-logloss:0.00306\n",
      "[390]\teval-logloss:0.16004\ttrain-logloss:0.00305\n",
      "[391]\teval-logloss:0.16004\ttrain-logloss:0.00304\n",
      "[392]\teval-logloss:0.16008\ttrain-logloss:0.00303\n",
      "[393]\teval-logloss:0.16005\ttrain-logloss:0.00302\n",
      "[394]\teval-logloss:0.16008\ttrain-logloss:0.00301\n",
      "[395]\teval-logloss:0.16009\ttrain-logloss:0.00300\n",
      "[396]\teval-logloss:0.16017\ttrain-logloss:0.00299\n",
      "[397]\teval-logloss:0.16019\ttrain-logloss:0.00298\n",
      "[398]\teval-logloss:0.16026\ttrain-logloss:0.00297\n",
      "[399]\teval-logloss:0.16027\ttrain-logloss:0.00296\n",
      "[400]\teval-logloss:0.16029\ttrain-logloss:0.00296\n",
      "[401]\teval-logloss:0.16032\ttrain-logloss:0.00295\n",
      "[402]\teval-logloss:0.16036\ttrain-logloss:0.00294\n",
      "[403]\teval-logloss:0.16034\ttrain-logloss:0.00293\n",
      "[404]\teval-logloss:0.16041\ttrain-logloss:0.00292\n",
      "[405]\teval-logloss:0.16044\ttrain-logloss:0.00291\n",
      "[406]\teval-logloss:0.16047\ttrain-logloss:0.00290\n",
      "[407]\teval-logloss:0.16049\ttrain-logloss:0.00289\n",
      "[408]\teval-logloss:0.16057\ttrain-logloss:0.00288\n",
      "[409]\teval-logloss:0.16058\ttrain-logloss:0.00288\n",
      "[410]\teval-logloss:0.16064\ttrain-logloss:0.00287\n",
      "[411]\teval-logloss:0.16069\ttrain-logloss:0.00286\n",
      "[412]\teval-logloss:0.16069\ttrain-logloss:0.00286\n",
      "[413]\teval-logloss:0.16071\ttrain-logloss:0.00285\n",
      "[414]\teval-logloss:0.16073\ttrain-logloss:0.00284\n",
      "[415]\teval-logloss:0.16074\ttrain-logloss:0.00283\n",
      "[416]\teval-logloss:0.16076\ttrain-logloss:0.00283\n",
      "[417]\teval-logloss:0.16081\ttrain-logloss:0.00282\n",
      "[418]\teval-logloss:0.16082\ttrain-logloss:0.00281\n",
      "[419]\teval-logloss:0.16083\ttrain-logloss:0.00281\n",
      "[420]\teval-logloss:0.16080\ttrain-logloss:0.00280\n",
      "[421]\teval-logloss:0.16080\ttrain-logloss:0.00279\n",
      "[422]\teval-logloss:0.16079\ttrain-logloss:0.00278\n",
      "[423]\teval-logloss:0.16082\ttrain-logloss:0.00277\n",
      "[424]\teval-logloss:0.16084\ttrain-logloss:0.00276\n",
      "[425]\teval-logloss:0.16086\ttrain-logloss:0.00275\n",
      "[426]\teval-logloss:0.16092\ttrain-logloss:0.00275\n",
      "[427]\teval-logloss:0.16098\ttrain-logloss:0.00274\n",
      "[428]\teval-logloss:0.16101\ttrain-logloss:0.00273\n",
      "[429]\teval-logloss:0.16103\ttrain-logloss:0.00273\n",
      "[430]\teval-logloss:0.16103\ttrain-logloss:0.00272\n",
      "[431]\teval-logloss:0.16106\ttrain-logloss:0.00271\n",
      "[432]\teval-logloss:0.16104\ttrain-logloss:0.00270\n",
      "[433]\teval-logloss:0.16107\ttrain-logloss:0.00269\n",
      "[434]\teval-logloss:0.16109\ttrain-logloss:0.00269\n",
      "[435]\teval-logloss:0.16111\ttrain-logloss:0.00268\n",
      "[436]\teval-logloss:0.16116\ttrain-logloss:0.00267\n",
      "[437]\teval-logloss:0.16119\ttrain-logloss:0.00266\n",
      "[438]\teval-logloss:0.16120\ttrain-logloss:0.00265\n",
      "[439]\teval-logloss:0.16121\ttrain-logloss:0.00265\n",
      "[440]\teval-logloss:0.16123\ttrain-logloss:0.00264\n",
      "[441]\teval-logloss:0.16126\ttrain-logloss:0.00264\n",
      "[442]\teval-logloss:0.16125\ttrain-logloss:0.00263\n",
      "[443]\teval-logloss:0.16129\ttrain-logloss:0.00262\n",
      "[444]\teval-logloss:0.16132\ttrain-logloss:0.00261\n",
      "[445]\teval-logloss:0.16135\ttrain-logloss:0.00261\n",
      "[446]\teval-logloss:0.16137\ttrain-logloss:0.00260\n",
      "[447]\teval-logloss:0.16137\ttrain-logloss:0.00259\n",
      "[448]\teval-logloss:0.16138\ttrain-logloss:0.00259\n",
      "[449]\teval-logloss:0.16142\ttrain-logloss:0.00258\n",
      "[450]\teval-logloss:0.16142\ttrain-logloss:0.00257\n",
      "[451]\teval-logloss:0.16146\ttrain-logloss:0.00256\n",
      "[452]\teval-logloss:0.16148\ttrain-logloss:0.00256\n",
      "[453]\teval-logloss:0.16149\ttrain-logloss:0.00255\n",
      "[454]\teval-logloss:0.16153\ttrain-logloss:0.00254\n",
      "[455]\teval-logloss:0.16157\ttrain-logloss:0.00254\n",
      "[456]\teval-logloss:0.16162\ttrain-logloss:0.00253\n",
      "[457]\teval-logloss:0.16167\ttrain-logloss:0.00253\n",
      "[458]\teval-logloss:0.16171\ttrain-logloss:0.00252\n",
      "[459]\teval-logloss:0.16173\ttrain-logloss:0.00251\n",
      "[460]\teval-logloss:0.16175\ttrain-logloss:0.00250\n",
      "[461]\teval-logloss:0.16177\ttrain-logloss:0.00250\n",
      "[462]\teval-logloss:0.16181\ttrain-logloss:0.00249\n",
      "[463]\teval-logloss:0.16184\ttrain-logloss:0.00249\n",
      "[464]\teval-logloss:0.16184\ttrain-logloss:0.00248\n",
      "[465]\teval-logloss:0.16186\ttrain-logloss:0.00248\n",
      "[466]\teval-logloss:0.16187\ttrain-logloss:0.00247\n",
      "[467]\teval-logloss:0.16188\ttrain-logloss:0.00247\n",
      "[468]\teval-logloss:0.16192\ttrain-logloss:0.00246\n",
      "[469]\teval-logloss:0.16195\ttrain-logloss:0.00245\n",
      "[470]\teval-logloss:0.16198\ttrain-logloss:0.00244\n",
      "[471]\teval-logloss:0.16200\ttrain-logloss:0.00244\n",
      "[472]\teval-logloss:0.16200\ttrain-logloss:0.00243\n",
      "[473]\teval-logloss:0.16205\ttrain-logloss:0.00243\n",
      "[474]\teval-logloss:0.16207\ttrain-logloss:0.00242\n",
      "[475]\teval-logloss:0.16209\ttrain-logloss:0.00242\n",
      "[476]\teval-logloss:0.16211\ttrain-logloss:0.00241\n",
      "[477]\teval-logloss:0.16210\ttrain-logloss:0.00240\n",
      "[478]\teval-logloss:0.16212\ttrain-logloss:0.00239\n",
      "[479]\teval-logloss:0.16211\ttrain-logloss:0.00239\n",
      "[480]\teval-logloss:0.16211\ttrain-logloss:0.00238\n",
      "[481]\teval-logloss:0.16210\ttrain-logloss:0.00238\n",
      "[482]\teval-logloss:0.16215\ttrain-logloss:0.00237\n",
      "[483]\teval-logloss:0.16218\ttrain-logloss:0.00237\n",
      "[484]\teval-logloss:0.16221\ttrain-logloss:0.00236\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[485]\teval-logloss:0.16223\ttrain-logloss:0.00235\n",
      "[486]\teval-logloss:0.16227\ttrain-logloss:0.00235\n",
      "[487]\teval-logloss:0.16231\ttrain-logloss:0.00234\n",
      "[488]\teval-logloss:0.16234\ttrain-logloss:0.00234\n",
      "[489]\teval-logloss:0.16231\ttrain-logloss:0.00233\n",
      "[490]\teval-logloss:0.16233\ttrain-logloss:0.00233\n",
      "[491]\teval-logloss:0.16234\ttrain-logloss:0.00232\n",
      "[492]\teval-logloss:0.16236\ttrain-logloss:0.00232\n",
      "[493]\teval-logloss:0.16238\ttrain-logloss:0.00231\n",
      "[494]\teval-logloss:0.16241\ttrain-logloss:0.00230\n",
      "[495]\teval-logloss:0.16244\ttrain-logloss:0.00230\n",
      "[496]\teval-logloss:0.16245\ttrain-logloss:0.00229\n",
      "[497]\teval-logloss:0.16249\ttrain-logloss:0.00229\n",
      "[498]\teval-logloss:0.16253\ttrain-logloss:0.00228\n",
      "[499]\teval-logloss:0.16259\ttrain-logloss:0.00228\n",
      "[01:04:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"n_estimators\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[01:04:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"n_estimators\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[01:04:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"n_estimators\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[01:04:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"n_estimators\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[01:04:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"n_estimators\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[01:04:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"n_estimators\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[01:04:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"n_estimators\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[01:04:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"n_estimators\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[01:04:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"n_estimators\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[01:04:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"n_estimators\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-auc:0.95980+0.00074\ttest-auc:0.88977+0.00255\n",
      "[1]\ttrain-auc:0.98416+0.00176\ttest-auc:0.92224+0.00826\n",
      "[2]\ttrain-auc:0.99181+0.00089\ttest-auc:0.93887+0.00836\n",
      "[3]\ttrain-auc:0.99473+0.00057\ttest-auc:0.94663+0.00613\n",
      "[4]\ttrain-auc:0.99640+0.00042\ttest-auc:0.95290+0.00389\n",
      "[5]\ttrain-auc:0.99731+0.00036\ttest-auc:0.95725+0.00314\n",
      "[6]\ttrain-auc:0.99794+0.00046\ttest-auc:0.96032+0.00351\n",
      "[7]\ttrain-auc:0.99837+0.00047\ttest-auc:0.96217+0.00314\n",
      "[8]\ttrain-auc:0.99872+0.00050\ttest-auc:0.96394+0.00341\n",
      "[9]\ttrain-auc:0.99899+0.00053\ttest-auc:0.96595+0.00250\n",
      "[10]\ttrain-auc:0.99922+0.00058\ttest-auc:0.96751+0.00188\n",
      "[11]\ttrain-auc:0.99935+0.00060\ttest-auc:0.96830+0.00225\n",
      "[12]\ttrain-auc:0.99947+0.00060\ttest-auc:0.96907+0.00206\n",
      "[13]\ttrain-auc:0.99956+0.00058\ttest-auc:0.97000+0.00186\n",
      "[14]\ttrain-auc:0.99964+0.00058\ttest-auc:0.97062+0.00173\n",
      "[15]\ttrain-auc:0.99970+0.00057\ttest-auc:0.97119+0.00180\n",
      "[16]\ttrain-auc:0.99976+0.00055\ttest-auc:0.97197+0.00176\n",
      "[17]\ttrain-auc:0.99981+0.00054\ttest-auc:0.97232+0.00160\n",
      "[18]\ttrain-auc:0.99984+0.00055\ttest-auc:0.97265+0.00153\n",
      "[19]\ttrain-auc:0.99987+0.00055\ttest-auc:0.97320+0.00147\n",
      "[20]\ttrain-auc:0.99990+0.00054\ttest-auc:0.97353+0.00142\n",
      "[21]\ttrain-auc:0.99993+0.00054\ttest-auc:0.97392+0.00128\n",
      "[22]\ttrain-auc:0.99996+0.00053\ttest-auc:0.97430+0.00130\n",
      "[23]\ttrain-auc:0.99998+0.00053\ttest-auc:0.97469+0.00133\n",
      "[24]\ttrain-auc:1.00000+0.00053\ttest-auc:0.97496+0.00136\n",
      "[25]\ttrain-auc:1.00001+0.00053\ttest-auc:0.97538+0.00136\n",
      "[26]\ttrain-auc:1.00003+0.00053\ttest-auc:0.97557+0.00146\n",
      "[27]\ttrain-auc:1.00004+0.00053\ttest-auc:0.97588+0.00142\n",
      "[28]\ttrain-auc:1.00005+0.00053\ttest-auc:0.97611+0.00143\n",
      "[29]\ttrain-auc:1.00006+0.00053\ttest-auc:0.97633+0.00128\n",
      "[30]\ttrain-auc:1.00007+0.00053\ttest-auc:0.97645+0.00123\n",
      "[31]\ttrain-auc:1.00008+0.00053\ttest-auc:0.97660+0.00121\n",
      "[32]\ttrain-auc:1.00009+0.00053\ttest-auc:0.97691+0.00119\n",
      "[33]\ttrain-auc:1.00009+0.00053\ttest-auc:0.97718+0.00121\n",
      "[34]\ttrain-auc:1.00010+0.00053\ttest-auc:0.97730+0.00125\n",
      "[35]\ttrain-auc:1.00010+0.00053\ttest-auc:0.97748+0.00124\n",
      "[36]\ttrain-auc:1.00011+0.00054\ttest-auc:0.97771+0.00115\n",
      "[37]\ttrain-auc:1.00011+0.00053\ttest-auc:0.97797+0.00114\n",
      "[38]\ttrain-auc:1.00012+0.00053\ttest-auc:0.97819+0.00110\n",
      "[39]\ttrain-auc:1.00012+0.00053\ttest-auc:0.97838+0.00106\n",
      "[40]\ttrain-auc:1.00013+0.00053\ttest-auc:0.97848+0.00105\n",
      "[41]\ttrain-auc:1.00013+0.00053\ttest-auc:0.97860+0.00110\n",
      "[42]\ttrain-auc:1.00013+0.00053\ttest-auc:0.97874+0.00112\n",
      "[43]\ttrain-auc:1.00013+0.00053\ttest-auc:0.97885+0.00114\n",
      "[44]\ttrain-auc:1.00013+0.00053\ttest-auc:0.97896+0.00116\n",
      "[45]\ttrain-auc:1.00013+0.00053\ttest-auc:0.97914+0.00115\n",
      "[46]\ttrain-auc:1.00013+0.00053\ttest-auc:0.97922+0.00114\n",
      "[47]\ttrain-auc:1.00014+0.00053\ttest-auc:0.97936+0.00113\n",
      "[48]\ttrain-auc:1.00014+0.00053\ttest-auc:0.97949+0.00110\n",
      "[49]\ttrain-auc:1.00014+0.00053\ttest-auc:0.97963+0.00109\n",
      "[50]\ttrain-auc:1.00014+0.00053\ttest-auc:0.97976+0.00110\n",
      "[51]\ttrain-auc:1.00014+0.00053\ttest-auc:0.97988+0.00108\n",
      "[52]\ttrain-auc:1.00014+0.00053\ttest-auc:0.97998+0.00106\n",
      "[53]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98009+0.00103\n",
      "[54]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98020+0.00103\n",
      "[55]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98030+0.00102\n",
      "[56]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98036+0.00100\n",
      "[57]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98045+0.00095\n",
      "[58]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98055+0.00096\n",
      "[59]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98064+0.00097\n",
      "[60]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98073+0.00097\n",
      "[61]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98082+0.00098\n",
      "[62]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98088+0.00099\n",
      "[63]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98097+0.00095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[64]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98105+0.00092\n",
      "[65]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98112+0.00092\n",
      "[66]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98118+0.00092\n",
      "[67]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98126+0.00092\n",
      "[68]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98131+0.00092\n",
      "[69]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98135+0.00090\n",
      "[70]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98142+0.00089\n",
      "[71]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98149+0.00086\n",
      "[72]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98152+0.00084\n",
      "[73]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98156+0.00083\n",
      "[74]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98161+0.00082\n",
      "[75]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98167+0.00080\n",
      "[76]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98172+0.00080\n",
      "[77]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98177+0.00079\n",
      "[78]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98180+0.00078\n",
      "[79]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98187+0.00077\n",
      "[80]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98189+0.00076\n",
      "[81]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98193+0.00078\n",
      "[82]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98197+0.00079\n",
      "[83]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98204+0.00077\n",
      "[84]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98209+0.00079\n",
      "[85]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98212+0.00079\n",
      "[86]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98216+0.00080\n",
      "[87]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98218+0.00081\n",
      "[88]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98221+0.00080\n",
      "[89]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98224+0.00079\n",
      "[90]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98229+0.00080\n",
      "[91]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98233+0.00078\n",
      "[92]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98235+0.00078\n",
      "[93]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98238+0.00077\n",
      "[94]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98241+0.00076\n",
      "[95]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98242+0.00076\n",
      "[96]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98245+0.00076\n",
      "[97]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98248+0.00076\n",
      "[98]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98251+0.00075\n",
      "[99]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98254+0.00075\n",
      "[100]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98257+0.00075\n",
      "[101]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98260+0.00075\n",
      "[102]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98264+0.00076\n",
      "[103]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98267+0.00075\n",
      "[104]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98269+0.00073\n",
      "[105]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98272+0.00073\n",
      "[106]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98274+0.00074\n",
      "[107]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98276+0.00074\n",
      "[108]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98279+0.00074\n",
      "[109]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98281+0.00073\n",
      "[110]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98283+0.00073\n",
      "[111]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98284+0.00073\n",
      "[112]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98286+0.00073\n",
      "[113]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98288+0.00073\n",
      "[114]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98290+0.00072\n",
      "[115]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98292+0.00072\n",
      "[116]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98294+0.00071\n",
      "[117]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98297+0.00072\n",
      "[118]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98299+0.00074\n",
      "[119]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98302+0.00074\n",
      "[120]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98304+0.00075\n",
      "[121]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98305+0.00074\n",
      "[122]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98307+0.00075\n",
      "[123]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98309+0.00074\n",
      "[124]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98309+0.00075\n",
      "[125]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98310+0.00073\n",
      "[126]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98312+0.00072\n",
      "[127]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98314+0.00073\n",
      "[128]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98316+0.00073\n",
      "[129]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98317+0.00073\n",
      "[130]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98319+0.00072\n",
      "[131]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98322+0.00071\n",
      "[132]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98324+0.00071\n",
      "[133]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98326+0.00069\n",
      "[134]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98326+0.00069\n",
      "[135]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98329+0.00069\n",
      "[136]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98331+0.00069\n",
      "[137]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98332+0.00068\n",
      "[138]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98333+0.00069\n",
      "[139]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98335+0.00069\n",
      "[140]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98335+0.00069\n",
      "[141]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98336+0.00069\n",
      "[142]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98338+0.00069\n",
      "[143]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98340+0.00069\n",
      "[144]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98341+0.00068\n",
      "[145]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98342+0.00069\n",
      "[146]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98343+0.00068\n",
      "[147]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98344+0.00068\n",
      "[148]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98344+0.00068\n",
      "[149]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98346+0.00068\n",
      "[150]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98346+0.00068\n",
      "[151]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98347+0.00069\n",
      "[152]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98348+0.00069\n",
      "[153]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98350+0.00069\n",
      "[154]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98350+0.00069\n",
      "[155]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98351+0.00069\n",
      "[156]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98351+0.00069\n",
      "[157]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98353+0.00069\n",
      "[158]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98354+0.00069\n",
      "[159]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98354+0.00069\n",
      "[160]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98356+0.00068\n",
      "[161]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98356+0.00068\n",
      "[162]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98358+0.00068\n",
      "[163]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98358+0.00069\n",
      "[164]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98359+0.00069\n",
      "[165]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98360+0.00068\n",
      "[166]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98362+0.00067\n",
      "[167]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98362+0.00067\n",
      "[168]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98362+0.00068\n",
      "[169]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98363+0.00068\n",
      "[170]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98364+0.00069\n",
      "[171]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98365+0.00069\n",
      "[172]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98365+0.00069\n",
      "[173]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98366+0.00069\n",
      "[174]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98367+0.00069\n",
      "[175]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98369+0.00069\n",
      "[176]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98369+0.00069\n",
      "[177]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98371+0.00069\n",
      "[178]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98371+0.00069\n",
      "[179]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98372+0.00069\n",
      "[180]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98372+0.00069\n",
      "[181]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98374+0.00069\n",
      "[182]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98375+0.00070\n",
      "[183]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98375+0.00070\n",
      "[184]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98375+0.00070\n",
      "[185]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98376+0.00070\n",
      "[186]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98376+0.00070\n",
      "[187]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98377+0.00070\n",
      "[188]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98377+0.00070\n",
      "[189]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98378+0.00069\n",
      "[190]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98378+0.00070\n",
      "[191]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98378+0.00070\n",
      "[192]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98379+0.00070\n",
      "[193]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98380+0.00070\n",
      "[194]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98381+0.00069\n",
      "[195]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98381+0.00069\n",
      "[196]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98381+0.00069\n",
      "[197]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98382+0.00069\n",
      "[198]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98382+0.00069\n",
      "[199]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98382+0.00069\n",
      "[200]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98383+0.00069\n",
      "[201]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98384+0.00068\n",
      "[202]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98384+0.00068\n",
      "[203]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98385+0.00069\n",
      "[204]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98385+0.00069\n",
      "[205]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98385+0.00069\n",
      "[206]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98386+0.00069\n",
      "[207]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98387+0.00069\n",
      "[208]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98387+0.00069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[209]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98388+0.00069\n",
      "[210]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98388+0.00069\n",
      "[211]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98389+0.00069\n",
      "[212]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98390+0.00070\n",
      "[213]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98390+0.00069\n",
      "[214]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98391+0.00069\n",
      "[215]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98392+0.00069\n",
      "[216]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98393+0.00069\n",
      "[217]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98393+0.00069\n",
      "[218]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98394+0.00069\n",
      "[219]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98394+0.00069\n",
      "[220]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98395+0.00068\n",
      "[221]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98396+0.00068\n",
      "[222]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98396+0.00069\n",
      "[223]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98396+0.00069\n",
      "[224]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98397+0.00069\n",
      "[225]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98397+0.00068\n",
      "[226]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98397+0.00068\n",
      "[227]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98398+0.00068\n",
      "[228]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98398+0.00068\n",
      "[229]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98399+0.00068\n",
      "[230]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98399+0.00068\n",
      "[231]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98399+0.00068\n",
      "[232]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98399+0.00068\n",
      "[233]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98400+0.00068\n",
      "[234]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98400+0.00068\n",
      "[235]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98400+0.00067\n",
      "[236]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98402+0.00068\n",
      "[237]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98402+0.00068\n",
      "[238]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98402+0.00068\n",
      "[239]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98403+0.00067\n",
      "[240]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98404+0.00067\n",
      "[241]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98404+0.00068\n",
      "[242]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98405+0.00068\n",
      "[243]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98404+0.00069\n",
      "[244]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98405+0.00069\n",
      "[245]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98405+0.00069\n",
      "[246]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98404+0.00068\n",
      "[247]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98405+0.00068\n",
      "[248]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98405+0.00069\n",
      "[249]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98405+0.00070\n",
      "[250]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98406+0.00070\n",
      "[251]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98407+0.00070\n",
      "[252]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98408+0.00071\n",
      "[253]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98408+0.00070\n",
      "[254]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98408+0.00070\n",
      "[255]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98408+0.00070\n",
      "[256]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98408+0.00070\n",
      "[257]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98408+0.00070\n",
      "[258]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98409+0.00069\n",
      "[259]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98409+0.00069\n",
      "[260]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98410+0.00069\n",
      "[261]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98410+0.00068\n",
      "[262]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98410+0.00068\n",
      "[263]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98410+0.00068\n",
      "[264]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98410+0.00069\n",
      "[265]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98411+0.00068\n",
      "[266]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98411+0.00068\n",
      "[267]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98411+0.00068\n",
      "[268]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98411+0.00068\n",
      "[269]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98412+0.00068\n",
      "[270]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98412+0.00067\n",
      "[271]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98412+0.00068\n",
      "[272]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98412+0.00068\n",
      "[273]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98412+0.00068\n",
      "[274]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98412+0.00068\n",
      "[275]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98412+0.00068\n",
      "[276]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98411+0.00068\n",
      "[277]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98412+0.00068\n",
      "[278]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98412+0.00067\n",
      "[279]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98412+0.00068\n",
      "[280]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98412+0.00068\n",
      "[281]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98412+0.00068\n",
      "[282]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98412+0.00069\n",
      "[283]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98412+0.00068\n",
      "[284]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98413+0.00068\n",
      "[285]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98413+0.00068\n",
      "[286]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98413+0.00068\n",
      "[287]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98414+0.00069\n",
      "[288]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98414+0.00069\n",
      "[289]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98414+0.00069\n",
      "[290]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98414+0.00069\n",
      "[291]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98414+0.00068\n",
      "[292]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98415+0.00068\n",
      "[293]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98415+0.00068\n",
      "[294]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98415+0.00068\n",
      "[295]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98416+0.00068\n",
      "[296]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98416+0.00068\n",
      "[297]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98416+0.00068\n",
      "[298]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98417+0.00068\n",
      "[299]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98417+0.00068\n",
      "[300]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98417+0.00068\n",
      "[301]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98417+0.00068\n",
      "[302]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98417+0.00068\n",
      "[303]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98418+0.00068\n",
      "[304]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98417+0.00069\n",
      "[305]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98417+0.00068\n",
      "[306]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98418+0.00069\n",
      "[307]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98418+0.00069\n",
      "[308]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98418+0.00069\n",
      "[309]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98419+0.00069\n",
      "[310]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98419+0.00069\n",
      "[311]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98419+0.00069\n",
      "[312]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98419+0.00070\n",
      "[313]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98420+0.00070\n",
      "[314]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98420+0.00070\n",
      "[315]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98421+0.00070\n",
      "[316]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98421+0.00070\n",
      "[317]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98421+0.00070\n",
      "[318]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98421+0.00070\n",
      "[319]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98421+0.00070\n",
      "[320]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98422+0.00070\n",
      "[321]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98422+0.00070\n",
      "[322]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98422+0.00070\n",
      "[323]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98422+0.00070\n",
      "[324]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98422+0.00070\n",
      "[325]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98422+0.00069\n",
      "[326]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98422+0.00069\n",
      "[327]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98422+0.00070\n",
      "[328]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98422+0.00070\n",
      "[329]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98422+0.00070\n",
      "[330]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98422+0.00070\n",
      "[331]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98423+0.00070\n",
      "[332]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98423+0.00070\n",
      "[333]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98423+0.00070\n",
      "[334]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98423+0.00070\n",
      "[335]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98423+0.00070\n",
      "[336]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98423+0.00070\n",
      "[337]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98423+0.00070\n",
      "[338]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98423+0.00070\n",
      "[339]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98423+0.00070\n",
      "[340]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98423+0.00070\n",
      "[341]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98423+0.00070\n",
      "[342]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98423+0.00070\n",
      "[343]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98424+0.00070\n",
      "[344]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98424+0.00069\n",
      "[345]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98424+0.00069\n",
      "[346]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98424+0.00069\n",
      "[347]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98424+0.00069\n",
      "[348]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98425+0.00068\n",
      "[349]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98425+0.00068\n",
      "[350]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98425+0.00068\n",
      "[351]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98425+0.00068\n",
      "[352]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98425+0.00068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[353]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98425+0.00068\n",
      "[354]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98425+0.00068\n",
      "[355]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98425+0.00068\n",
      "[356]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98426+0.00068\n",
      "[357]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98426+0.00069\n",
      "[358]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98426+0.00069\n",
      "[359]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98426+0.00069\n",
      "[360]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98426+0.00069\n",
      "[361]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98426+0.00069\n",
      "[362]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98426+0.00069\n",
      "[363]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98426+0.00069\n",
      "[364]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98427+0.00069\n",
      "[365]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98427+0.00069\n",
      "[366]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98427+0.00069\n",
      "[367]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98427+0.00069\n",
      "[368]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98427+0.00069\n",
      "[369]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98427+0.00069\n",
      "[370]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98427+0.00069\n",
      "[371]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98428+0.00069\n",
      "[372]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98428+0.00069\n",
      "[373]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98428+0.00068\n",
      "[374]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98428+0.00068\n",
      "[375]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98428+0.00068\n",
      "[376]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98428+0.00068\n",
      "[377]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98428+0.00068\n",
      "[378]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98428+0.00068\n",
      "[379]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98429+0.00068\n",
      "[380]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98428+0.00068\n",
      "[381]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98428+0.00068\n",
      "[382]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98429+0.00068\n",
      "[383]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98429+0.00068\n",
      "[384]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98429+0.00068\n",
      "[385]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98430+0.00068\n",
      "[386]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98430+0.00068\n",
      "[387]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98430+0.00068\n",
      "[388]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98430+0.00068\n",
      "[389]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98430+0.00068\n",
      "[390]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98430+0.00068\n",
      "[391]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98431+0.00068\n",
      "[392]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98431+0.00068\n",
      "[393]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98431+0.00068\n",
      "[394]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98431+0.00068\n",
      "[395]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98432+0.00068\n",
      "[396]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98432+0.00068\n",
      "[397]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98432+0.00068\n",
      "[398]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98432+0.00068\n",
      "[399]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98433+0.00068\n",
      "[400]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98433+0.00068\n",
      "[401]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98433+0.00068\n",
      "[402]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98433+0.00068\n",
      "[403]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98433+0.00068\n",
      "[404]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98433+0.00068\n",
      "[405]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98433+0.00068\n",
      "[406]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98434+0.00068\n",
      "[407]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98434+0.00068\n",
      "[408]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98434+0.00068\n",
      "[409]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98434+0.00068\n",
      "[410]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98434+0.00068\n",
      "[411]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98434+0.00068\n",
      "[412]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98435+0.00068\n",
      "[413]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98435+0.00068\n",
      "[414]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98435+0.00068\n",
      "[415]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98435+0.00068\n",
      "[416]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98435+0.00068\n",
      "[417]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98435+0.00068\n",
      "[418]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98435+0.00068\n",
      "[419]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98435+0.00068\n",
      "[420]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98435+0.00068\n",
      "[421]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98435+0.00067\n",
      "[422]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98435+0.00068\n",
      "[423]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98435+0.00067\n",
      "[424]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98435+0.00068\n",
      "[425]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98435+0.00068\n",
      "[426]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98435+0.00068\n",
      "[427]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98436+0.00068\n",
      "[428]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98436+0.00068\n",
      "[429]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98436+0.00068\n",
      "[430]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98436+0.00068\n",
      "[431]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98436+0.00068\n",
      "[432]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98436+0.00068\n",
      "[433]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98437+0.00068\n",
      "[434]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98437+0.00068\n",
      "[435]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98437+0.00067\n",
      "[436]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98437+0.00067\n",
      "[437]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98437+0.00067\n",
      "[438]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98437+0.00068\n",
      "[439]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98437+0.00068\n",
      "[440]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98437+0.00068\n",
      "[441]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98437+0.00068\n",
      "[442]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98437+0.00068\n",
      "[443]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98438+0.00068\n",
      "[444]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98438+0.00068\n",
      "[445]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98438+0.00068\n",
      "[446]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98438+0.00068\n",
      "[447]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98438+0.00068\n",
      "[448]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98438+0.00068\n",
      "[449]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98439+0.00068\n",
      "[450]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98439+0.00068\n",
      "[451]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98439+0.00068\n",
      "[452]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98439+0.00068\n",
      "[453]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98439+0.00068\n",
      "[454]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98439+0.00068\n",
      "[455]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98439+0.00068\n",
      "[456]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98439+0.00067\n",
      "[457]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98439+0.00068\n",
      "[458]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98439+0.00068\n",
      "[459]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98439+0.00068\n",
      "[460]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98439+0.00068\n",
      "[461]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98439+0.00068\n",
      "[462]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98439+0.00068\n",
      "[463]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98439+0.00068\n",
      "[464]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98439+0.00067\n",
      "[465]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98440+0.00067\n",
      "[466]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98440+0.00067\n",
      "[467]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98440+0.00067\n",
      "[468]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98440+0.00067\n",
      "[469]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98440+0.00067\n",
      "[470]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98440+0.00067\n",
      "[471]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98440+0.00067\n",
      "[472]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98440+0.00067\n",
      "[473]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98440+0.00067\n",
      "[474]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98440+0.00067\n",
      "[475]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98439+0.00067\n",
      "[476]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98440+0.00067\n",
      "[477]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98439+0.00067\n",
      "[478]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98440+0.00067\n",
      "[479]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98440+0.00068\n",
      "[480]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98440+0.00068\n",
      "[481]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98440+0.00068\n",
      "[482]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98440+0.00068\n",
      "[483]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98440+0.00068\n",
      "[484]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98440+0.00067\n",
      "[485]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98440+0.00067\n",
      "[486]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98441+0.00067\n",
      "[487]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98440+0.00067\n",
      "[488]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98441+0.00067\n",
      "[489]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98441+0.00067\n",
      "[490]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98441+0.00067\n",
      "[491]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98441+0.00067\n",
      "[492]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98441+0.00067\n",
      "[493]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98441+0.00067\n",
      "[494]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98441+0.00067\n",
      "[495]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98441+0.00067\n",
      "[496]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98441+0.00067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[497]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98441+0.00067\n",
      "[498]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98442+0.00067\n",
      "[499]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98442+0.00068\n",
      "[500]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98442+0.00068\n",
      "[501]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98442+0.00068\n",
      "[502]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98442+0.00068\n",
      "[503]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98442+0.00068\n",
      "[504]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98442+0.00068\n",
      "[505]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98442+0.00068\n",
      "[506]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98442+0.00068\n",
      "[507]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98443+0.00068\n",
      "[508]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98443+0.00068\n",
      "[509]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98443+0.00068\n",
      "[510]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98443+0.00068\n",
      "[511]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98443+0.00067\n",
      "[512]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98443+0.00068\n",
      "[513]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98443+0.00068\n",
      "[514]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98443+0.00068\n",
      "[515]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98443+0.00068\n",
      "[516]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98443+0.00068\n",
      "[517]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98443+0.00068\n",
      "[518]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98443+0.00068\n",
      "[519]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98444+0.00067\n",
      "[520]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98444+0.00067\n",
      "[521]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98444+0.00067\n",
      "[522]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98444+0.00067\n",
      "[523]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98444+0.00067\n",
      "[524]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98444+0.00067\n",
      "[525]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98444+0.00067\n",
      "[526]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98444+0.00067\n",
      "[527]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98444+0.00067\n",
      "[528]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98444+0.00067\n",
      "[529]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98444+0.00068\n",
      "[530]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98444+0.00068\n",
      "[531]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98444+0.00067\n",
      "[532]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98444+0.00067\n",
      "[533]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98445+0.00067\n",
      "[534]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98445+0.00067\n",
      "[535]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98444+0.00067\n",
      "[536]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98444+0.00067\n",
      "[537]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98444+0.00067\n",
      "[538]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98444+0.00067\n",
      "[539]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98444+0.00067\n",
      "[540]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98445+0.00067\n",
      "[541]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98445+0.00067\n",
      "[542]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98445+0.00067\n",
      "[543]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98445+0.00067\n",
      "[544]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98445+0.00068\n",
      "[545]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98445+0.00067\n",
      "[546]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98445+0.00067\n",
      "[547]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98445+0.00067\n",
      "[548]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98445+0.00067\n",
      "[549]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98445+0.00067\n",
      "[550]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98445+0.00067\n",
      "[551]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98445+0.00067\n",
      "[552]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98445+0.00067\n",
      "[553]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98445+0.00067\n",
      "[554]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98446+0.00067\n",
      "[555]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98446+0.00067\n",
      "[556]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98446+0.00067\n",
      "[557]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98446+0.00067\n",
      "[558]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98446+0.00067\n",
      "[559]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98446+0.00067\n",
      "[560]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98446+0.00067\n",
      "[561]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98445+0.00067\n",
      "[562]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98446+0.00067\n",
      "[563]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98446+0.00067\n",
      "[564]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98446+0.00066\n",
      "[565]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98446+0.00067\n",
      "[566]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98446+0.00067\n",
      "[567]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98446+0.00067\n",
      "[568]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98446+0.00067\n",
      "[569]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98446+0.00067\n",
      "[570]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98446+0.00067\n",
      "[571]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98446+0.00066\n",
      "[572]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98446+0.00066\n",
      "[573]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98446+0.00067\n",
      "[574]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98446+0.00067\n",
      "[575]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98446+0.00066\n",
      "[576]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98446+0.00066\n",
      "[577]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98446+0.00066\n",
      "[578]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98446+0.00066\n",
      "[579]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98446+0.00066\n",
      "[580]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98446+0.00066\n",
      "[581]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98446+0.00066\n",
      "[582]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98446+0.00066\n",
      "[583]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98446+0.00066\n",
      "[584]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98446+0.00066\n",
      "[585]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98446+0.00066\n",
      "[586]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98446+0.00066\n",
      "[587]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98446+0.00066\n",
      "[588]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98446+0.00066\n",
      "[589]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98447+0.00066\n",
      "[590]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98447+0.00066\n",
      "[591]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98447+0.00066\n",
      "[592]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98447+0.00066\n",
      "[593]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98447+0.00066\n",
      "[594]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98447+0.00066\n",
      "[595]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98447+0.00066\n",
      "[596]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98447+0.00066\n",
      "[597]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98447+0.00066\n",
      "[598]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98447+0.00066\n",
      "[599]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98448+0.00067\n",
      "[600]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98448+0.00066\n",
      "[601]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98448+0.00066\n",
      "[602]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98448+0.00066\n",
      "[603]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98448+0.00067\n",
      "[604]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98448+0.00067\n",
      "[605]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98448+0.00067\n",
      "[606]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98448+0.00067\n",
      "[607]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98448+0.00067\n",
      "[608]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98448+0.00067\n",
      "[609]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98448+0.00067\n",
      "[610]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98448+0.00067\n",
      "[611]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98448+0.00067\n",
      "[612]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98448+0.00067\n",
      "[613]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98448+0.00067\n",
      "[614]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98448+0.00067\n",
      "[615]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98448+0.00067\n",
      "[616]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98448+0.00067\n",
      "[617]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98448+0.00067\n",
      "[618]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98448+0.00067\n",
      "[619]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98448+0.00067\n",
      "[620]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98448+0.00067\n",
      "[621]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98448+0.00067\n",
      "[622]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98448+0.00067\n",
      "[623]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98448+0.00066\n",
      "[624]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98448+0.00066\n",
      "[625]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98448+0.00066\n",
      "[626]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98448+0.00066\n",
      "[627]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98448+0.00066\n",
      "[628]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98448+0.00066\n",
      "[629]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98448+0.00066\n",
      "[630]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98448+0.00066\n",
      "[631]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98448+0.00066\n",
      "[632]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98448+0.00066\n",
      "[633]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98448+0.00066\n",
      "[634]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98448+0.00066\n",
      "[635]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98448+0.00066\n",
      "[636]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98448+0.00067\n",
      "[637]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98448+0.00067\n",
      "[638]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98448+0.00067\n",
      "[639]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98448+0.00067\n",
      "[640]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98449+0.00067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[641]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98449+0.00067\n",
      "[642]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98448+0.00067\n",
      "[643]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98448+0.00067\n",
      "[644]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98448+0.00067\n",
      "[645]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98448+0.00067\n",
      "[646]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98448+0.00066\n",
      "[647]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98448+0.00067\n",
      "[648]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98448+0.00066\n",
      "[649]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98448+0.00066\n",
      "[650]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98449+0.00066\n",
      "[651]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98448+0.00066\n",
      "[652]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98449+0.00066\n",
      "[653]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98449+0.00067\n",
      "[654]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98449+0.00067\n",
      "[655]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98449+0.00067\n",
      "[656]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98449+0.00067\n",
      "[657]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98449+0.00066\n",
      "[658]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98449+0.00066\n",
      "[659]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98449+0.00066\n",
      "[660]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98449+0.00066\n",
      "[661]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98449+0.00066\n",
      "[662]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98449+0.00066\n",
      "[663]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98449+0.00066\n",
      "[664]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98449+0.00066\n",
      "[665]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98449+0.00066\n",
      "[666]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98449+0.00066\n",
      "[667]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98449+0.00066\n",
      "[668]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98449+0.00066\n",
      "[669]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98449+0.00066\n",
      "[670]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98449+0.00067\n",
      "[671]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98449+0.00067\n",
      "[672]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98449+0.00067\n",
      "[673]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98449+0.00067\n",
      "[674]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98449+0.00067\n",
      "[675]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98449+0.00067\n",
      "[676]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98449+0.00067\n",
      "[677]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98449+0.00067\n",
      "[678]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98449+0.00067\n",
      "[679]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98449+0.00067\n",
      "[680]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98449+0.00067\n",
      "[681]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98449+0.00067\n",
      "[682]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98449+0.00067\n",
      "[683]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98449+0.00067\n",
      "[684]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00067\n",
      "[685]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00067\n",
      "[686]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00067\n",
      "[687]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00067\n",
      "[688]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00067\n",
      "[689]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00067\n",
      "[690]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00068\n",
      "[691]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00068\n",
      "[692]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00068\n",
      "[693]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00068\n",
      "[694]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00068\n",
      "[695]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00068\n",
      "[696]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00068\n",
      "[697]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00068\n",
      "[698]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00068\n",
      "[699]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00068\n",
      "[700]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00068\n",
      "[701]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00068\n",
      "[702]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00068\n",
      "[703]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00068\n",
      "[704]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00068\n",
      "[705]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00068\n",
      "[706]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00068\n",
      "[707]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00068\n",
      "[708]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00068\n",
      "[709]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00068\n",
      "[710]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00068\n",
      "[711]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00068\n",
      "[712]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00068\n",
      "[713]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00068\n",
      "[714]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00068\n",
      "[715]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00068\n",
      "[716]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00068\n",
      "[717]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00068\n",
      "[718]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00068\n",
      "[719]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00068\n",
      "[720]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00068\n",
      "[721]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00068\n",
      "[722]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00068\n",
      "[723]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00068\n",
      "[724]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00068\n",
      "[725]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00068\n",
      "[726]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00068\n",
      "[727]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00068\n",
      "[728]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00068\n",
      "[729]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00068\n",
      "[730]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00068\n",
      "[731]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00068\n",
      "[732]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00068\n",
      "[733]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00068\n",
      "[734]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00068\n",
      "[735]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00068\n",
      "[736]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00068\n",
      "[737]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00068\n",
      "[738]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00068\n",
      "[739]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00068\n",
      "[740]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00068\n",
      "[741]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00068\n",
      "[742]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00068\n",
      "[743]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00068\n",
      "[744]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00068\n",
      "[745]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00068\n",
      "[746]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00068\n",
      "[747]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00068\n",
      "[748]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00068\n",
      "[749]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00068\n",
      "[750]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00068\n",
      "[751]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00068\n",
      "[752]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00068\n",
      "[753]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00068\n",
      "[754]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00068\n",
      "[755]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00068\n",
      "[756]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00068\n",
      "[757]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00068\n",
      "[758]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00068\n",
      "[759]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00068\n",
      "[760]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00068\n",
      "[761]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00068\n",
      "[762]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00068\n",
      "[763]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00068\n",
      "[764]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00068\n",
      "[765]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00068\n",
      "[766]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00068\n",
      "[767]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00068\n",
      "[768]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00068\n",
      "[769]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00068\n",
      "[770]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00068\n",
      "[771]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00068\n",
      "[772]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00068\n",
      "[773]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00068\n",
      "[774]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00068\n",
      "[775]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00068\n",
      "[776]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00068\n",
      "[777]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00068\n",
      "[778]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00068\n",
      "[779]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00068\n",
      "[780]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00068\n",
      "[781]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00068\n",
      "[782]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00068\n",
      "[783]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00068\n",
      "[784]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[785]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00068\n",
      "[786]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00068\n",
      "[787]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00068\n",
      "[788]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00068\n",
      "[789]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00068\n",
      "[790]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00068\n",
      "[791]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00068\n",
      "[792]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00068\n",
      "[793]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00069\n",
      "[794]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00068\n",
      "[795]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00069\n",
      "[796]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00069\n",
      "[797]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00069\n",
      "[798]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00069\n",
      "[799]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00069\n",
      "[800]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00069\n",
      "[801]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00069\n",
      "[802]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[803]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[804]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00069\n",
      "[805]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00069\n",
      "[806]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00069\n",
      "[807]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00069\n",
      "[808]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00069\n",
      "[809]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00069\n",
      "[810]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00069\n",
      "[811]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00069\n",
      "[812]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[813]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[814]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[815]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00069\n",
      "[816]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[817]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[818]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[819]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[820]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[821]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[822]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[823]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[824]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[825]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[826]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[827]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[828]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[829]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[830]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[831]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[832]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[833]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[834]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[835]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[836]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[837]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[838]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[839]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[840]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[841]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00070\n",
      "[842]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00070\n",
      "[843]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00070\n",
      "[844]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[845]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00070\n",
      "[846]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00070\n",
      "[847]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00070\n",
      "[848]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[849]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[850]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[851]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[852]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00070\n",
      "[853]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[854]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[855]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[856]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[857]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00069\n",
      "[858]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00069\n",
      "[859]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00069\n",
      "[860]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00069\n",
      "[861]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[862]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00069\n",
      "[863]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00069\n",
      "[864]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00069\n",
      "[865]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[866]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[867]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[868]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[869]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[870]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[871]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[872]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[873]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[874]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[875]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[876]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[877]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[878]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[879]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00070\n",
      "[880]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00070\n",
      "[881]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[882]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[883]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[884]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[885]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00070\n",
      "[886]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[887]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00070\n",
      "[888]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00070\n",
      "[889]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00070\n",
      "[890]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00070\n",
      "[891]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[892]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[893]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00070\n",
      "[894]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00070\n",
      "[895]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[896]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00070\n",
      "[897]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[898]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00070\n",
      "[899]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[900]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[901]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[902]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[903]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[904]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[905]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[906]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[907]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[908]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[909]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[910]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00070\n",
      "[911]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[912]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[913]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[914]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[915]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[916]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[917]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[918]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[919]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[920]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[921]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[922]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[923]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[924]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[925]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[926]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[927]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[928]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[929]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[930]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00070\n",
      "[931]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00070\n",
      "[932]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00070\n",
      "[933]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00070\n",
      "[934]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00070\n",
      "[935]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00070\n",
      "[936]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00070\n",
      "[937]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[938]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00070\n",
      "[939]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[940]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00070\n",
      "[941]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[942]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00070\n",
      "[943]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00070\n",
      "[944]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[945]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[946]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[947]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[948]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[949]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[950]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[951]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[952]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[953]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[954]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[955]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[956]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[957]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[958]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[959]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[960]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[961]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[962]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[963]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[964]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[965]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[966]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[967]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[968]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[969]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[970]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[971]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[972]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[973]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[974]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[975]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[976]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00069\n",
      "[977]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00069\n",
      "[978]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00069\n",
      "[979]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[980]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[981]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[982]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[983]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[984]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[985]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00069\n",
      "[986]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00069\n",
      "[987]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98451+0.00069\n",
      "[988]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[989]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[990]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[991]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[992]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[993]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[994]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[995]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[996]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[997]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[998]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "[999]\ttrain-auc:1.00014+0.00053\ttest-auc:0.98450+0.00069\n",
      "--------------------\n",
      "Working on XGboost............\n",
      "XGBoost cv result      train-auc-mean  train-auc-std  test-auc-mean  test-auc-std\n",
      "0          0.959804       0.000737       0.889767      0.002548\n",
      "1          0.984162       0.001758       0.922237      0.008262\n",
      "2          0.991807       0.000890       0.938874      0.008364\n",
      "3          0.994730       0.000568       0.946632      0.006126\n",
      "4          0.996398       0.000419       0.952902      0.003892\n",
      "..              ...            ...            ...           ...\n",
      "995        1.000143       0.000532       0.984501      0.000690\n",
      "996        1.000143       0.000532       0.984502      0.000690\n",
      "997        1.000143       0.000532       0.984502      0.000692\n",
      "998        1.000143       0.000532       0.984501      0.000691\n",
      "999        1.000143       0.000533       0.984502      0.000691\n",
      "\n",
      "[1000 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "classifiers = ['lr', 'rnd_clf', 'xgbr']\n",
    "classif_eval_xgb = {}\n",
    "\n",
    "for classifier in classifiers:\n",
    "    if classifier == 'xgbr':\n",
    "            classifier = 'XGBoost'        \n",
    "            #classification_report, auc_mean, auc_mean = load_pred('xgbr', root)\n",
    "            classif_eval_xgb[classifier] = model_clf(train_scaled_features, test_scaled_features, y_train, y_test, 'xgbr')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c32df092",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import dump\n",
    "from pickle import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ec65493d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the evaluation reports dictionary, so I don't have to train the model all over again!\n",
    "dump(classif_eval, open('classif_eval.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2440a899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the evaluation reports dictionary, so I don't have to train the model all over again!\n",
    "dump(classif_eval_xgb, open('classif_eval_xgb.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8b8ebd4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression....\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.73      0.75     14227\n",
      "           1       0.63      0.69      0.66      9532\n",
      "\n",
      "    accuracy                           0.71     23759\n",
      "   macro avg       0.70      0.71      0.70     23759\n",
      "weighted avg       0.72      0.71      0.71     23759\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Classification report\n",
    "print('Logistic Regression....\\n\\n', classif_eval['logistic regression'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "15192f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic:\n",
      " AUC mean is 0.7593207660525269 and AUC Stdev is 0.0032171728697389596\n"
     ]
    }
   ],
   "source": [
    "#auc mean and stdev\n",
    "print('logistic:\\n AUC mean is {} and AUC Stdev is {}'\n",
    "      .format(classif_eval['logistic regression'][2],classif_eval['logistic regression'][3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "11360b64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10370,  3857],\n",
       "       [ 2980,  6552]], dtype=int64)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, classif_eval['logistic regression'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8afb105f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest....\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.96      0.94     14227\n",
      "           1       0.93      0.87      0.90      9532\n",
      "\n",
      "    accuracy                           0.92     23759\n",
      "   macro avg       0.92      0.91      0.92     23759\n",
      "weighted avg       0.92      0.92      0.92     23759\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Classification report\n",
    "print('Random Forest....\\n\\n', classif_eval['Random Forest'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "749c17de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest:\n",
      " AUC mean is 0.9742849561649581 and AUC Stdev is 0.0015544316238684562\n"
     ]
    }
   ],
   "source": [
    "#auc mean and stdev\n",
    "print('Random Forest:\\n AUC mean is {} and AUC Stdev is {}'\n",
    "      .format(classif_eval['Random Forest'][2],classif_eval['Random Forest'][3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33eff250",
   "metadata": {},
   "source": [
    "**Confusion Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c8d79214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[13623,   604],\n",
       "       [ 1262,  8270]], dtype=int64)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, classif_eval['Random Forest'][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323fb89f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "213991f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost....\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.95      0.96     14440\n",
      "           1       0.92      0.94      0.93      9319\n",
      "\n",
      "    accuracy                           0.95     23759\n",
      "   macro avg       0.94      0.95      0.94     23759\n",
      "weighted avg       0.95      0.95      0.95     23759\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Classification report\n",
    "print('XGBoost....\\n\\n', classif_eval_xgb['XGBoost'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6f1c10f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost:\n",
      " AUC mean is 1.0000523452000014 and AUC Stdev is 0.0014177528358769026\n"
     ]
    }
   ],
   "source": [
    "#auc mean and stdev\n",
    "print('XGBoost:\\n AUC mean is {} and AUC Stdev is {}'\n",
    "      .format(classif_eval_xgb['XGBoost'][2],classif_eval_xgb['XGBoost'][3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e803cd1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[13692,   535],\n",
       "       [  748,  8784]], dtype=int64)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, classif_eval_xgb['XGBoost'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4afa686",
   "metadata": {},
   "source": [
    "## Evaluating  the cost of prediction.  \n",
    "- False negative: 0 if 1 = USD1500.   \n",
    "- False positive: 1 if 0 = USD500.  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dfe511",
   "metadata": {},
   "source": [
    "## Evaluating metrics\n",
    "\n",
    "**Best Model**\n",
    "\n",
    "- Recall provides the number of True Positives that are correctly labelled. Therefore we want to maximize this metric since the cost of False negative is higher than the cost of false positive in our case. But not at a huge cost of Precision as we penalize fore False positive as well.\n",
    "    - Even though XGboost and Random Forest models have statistically similar Recall for the Class O (0.95 versus 0.96), XGboost has the highest Recall for Class 1 ((0.94 versus 0.87).  \n",
    "    - This tells use that, overall XGboost will better minimize the total cost of incorrect prediction for this case study.  \n",
    "    - We will soon calculate these actual cost below to verify this information.  \n",
    "    \n",
    "- Precision tells us the proportion of predicted positive labels that are actually belong to the positive class which is usually a good measure when the cost of False positive is high. \n",
    "    - We do want to maximize this but not at the expense of high Recall. \n",
    "\n",
    "**AUC:** \n",
    "XGBoost has the highest at almost 1.0! This shows that XGBoost is the best model that can achieve very high true positive rate for predicting bankruptcy.  \n",
    "This AUC value is slightly better than that of random forest while logistic regression has the worst AUC value at  0.76.  \n",
    "\n",
    "\n",
    "**Accuracies**  \n",
    "Since our data is nearly balanced overall accuracy can be used to decide the best model, therefore XGBoost achieved the highest overall accuracy(0.95) and so it is the best model in this case.\n",
    "\n",
    "**F1 score**\n",
    "- If we decide to look at F1 score which gives us the balance between Recall and Precision.  \n",
    "- The highest F1 score (0.96) for each class was achieved by XGBoost so it is the best model using this metric again.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8490078b",
   "metadata": {},
   "source": [
    "### Definition of terms:  \n",
    "- The confusion matrix is made up of:  \n",
    "TN = True Negative  \n",
    "TP = True Positie  \n",
    "FN = False Negative.  \n",
    "FP = False Positive.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3a9f768c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[13692,   535],\n",
       "       [  748,  8784]], dtype=int64)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm_xgb=confusion_matrix(y_test, classif_eval_xgb['XGBoost'][1])\n",
    "cm_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5d589ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "TN=cm_xgb[0][0]\n",
    "FN=cm_xgb[1][0]\n",
    "TP=cm_xgb[1][1]\n",
    "FP=cm_xgb[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c1076fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outcome values : \n",
      " 8784 748 535 13692\n"
     ]
    }
   ],
   "source": [
    "print('Outcome values : \\n', TP, FN, FP, TN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "cdfcd6aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9459994107496107"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(TP+TN)/(TP+FP+FN+TN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3f13495e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total error cost:1389500\n"
     ]
    }
   ],
   "source": [
    "false_neg_cost = FN*1500\n",
    "false_pos_cost= FP*500\n",
    "total_error_cost_xgb = false_pos_cost+false_neg_cost\n",
    "print('total error cost:{}'.format(total_error_cost_xgb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8289baa3",
   "metadata": {},
   "source": [
    "## Random Forest  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6d90deab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm_rf=confusion_matrix(y_test, classif_eval['Random Forest'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "15a15426",
   "metadata": {},
   "outputs": [],
   "source": [
    "TN=cm_rf[0][0]\n",
    "FN=cm_rf[1][0]\n",
    "TP=cm_rf[1][1]\n",
    "FP=cm_rf[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "af83e47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total error cost:2195000\n"
     ]
    }
   ],
   "source": [
    "false_neg_cost = FN*1500\n",
    "false_pos_cost= FP*500\n",
    "total_error_cost_rf = false_pos_cost+false_neg_cost\n",
    "print('total error cost:{}'.format(total_error_cost_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6928174",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31965e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c351f7b",
   "metadata": {},
   "source": [
    "## Logistic Regression.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "744e7f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm_lr=confusion_matrix(y_test, classif_eval['logistic regression'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0ed44fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "TN=cm_lr[0][0]\n",
    "FN=cm_lr[1][0]\n",
    "TP=cm_lr[1][1]\n",
    "FP=cm_lr[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "14af1d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total error cost:$6,398,500\n"
     ]
    }
   ],
   "source": [
    "false_neg_cost = FN*1500\n",
    "false_pos_cost= FP*500\n",
    "total_error_cost_lr = false_pos_cost+false_neg_cost\n",
    "print('total error cost:${:,}'.format(total_error_cost_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea9d81c",
   "metadata": {},
   "source": [
    "## Obtain the model that has the lowest cost of prediction error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8f44193c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowest cost is XGBoost model: USD$1,389,500\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#Argmin will give us the name of the item with the minimum value in an array.  \n",
    "# In this case that will be the name of the estimator.  \n",
    "\n",
    "if np.argmin([total_error_cost_xgb, total_error_cost_rf, total_error_cost_lr]) == 0:\n",
    "    print('Lowest cost is XGBoost model: USD${:,}'.format(total_error_cost_xgb))\n",
    "elif np.argmin([total_error_cost_xgb, total_error_cost_rf, total_error_cost_lr]) == 1:\n",
    "    print ('Lowest cost is Random Forest model: USD${:,}'.format(total_error_cost_rf))\n",
    "    \n",
    "elif np.argmin([total_error_cost_xgb, total_error_cost_rf, total_error_cost_lr]) == 2:\n",
    "    print ('Lowest cost is Logistic Regression model: USD${:,}'.format(total_error_cost_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289379db",
   "metadata": {},
   "source": [
    "**As seen above, the XGBoost model resulted in the lowesrt cost of prediction error.**\n",
    "- Therefore we will adopt the XGBoost model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebb031f",
   "metadata": {},
   "source": [
    "## Feature importance.  \n",
    "- We have no information about the features therefore we may not be able to interpret feature importance scores.  \n",
    "- However we can display them here.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0367322",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360c4b2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce14d6b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b8c58e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ef6dc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d3a8a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd8e591",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf5af87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9870b25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766b146b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
